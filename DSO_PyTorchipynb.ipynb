{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cro3Kw7UjAgH",
        "outputId": "f5b593f3-f1d2-4624-95c7-16ad903cfbb0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install gplearn scikit-learn\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qCB6_a9ZjHn7",
        "outputId": "cb6fcaca-4355-42d2-909b-e98a4fdf48e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gplearn\n",
            "  Downloading gplearn-0.4.2-py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: joblib>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from gplearn) (1.4.0)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.4)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.4.0)\n",
            "Installing collected packages: gplearn\n",
            "Successfully installed gplearn-0.4.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Paths to the CSV files\n",
        "train_features_path = '/content/drive/MyDrive/ngsim 2/ngsim_v_dt=1/train_feature.csv'\n",
        "train_labels_path = '/content/drive/MyDrive/ngsim 2/ngsim_v_dt=1/train_label.csv'\n",
        "validation_features_path = '/content/drive/MyDrive/ngsim 2/ngsim_v_dt=1/validation_feature.csv'\n",
        "validation_labels_path = '/content/drive/MyDrive/ngsim 2/ngsim_v_dt=1/validation_label.csv'"
      ],
      "metadata": {
        "id": "NcQBKPtqjN1P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "mSafqqvZjZk9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the CSV files without headers, since we will set column names manually\n",
        "train_features_df = pd.read_csv(train_features_path, header=None)\n",
        "train_labels_df = pd.read_csv(train_labels_path, header=None)\n",
        "validation_features_df = pd.read_csv(validation_features_path, header=None)\n",
        "validation_labels_df = pd.read_csv(validation_labels_path, header=None)"
      ],
      "metadata": {
        "id": "JcJCyu3_jRJH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set column names\n",
        "train_features_df.columns = ['dx', 'dv', 'v']\n",
        "train_labels_df.columns = ['v_next']\n",
        "validation_features_df.columns = ['dx', 'dv', 'v']\n",
        "validation_labels_df.columns = ['v_next']\n"
      ],
      "metadata": {
        "id": "6AiULOFzjWzb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Concatenate features and labels\n",
        "train_df = pd.concat([train_features_df, train_labels_df], axis=1)\n",
        "validation_df = pd.concat([validation_features_df, validation_labels_df], axis=1)"
      ],
      "metadata": {
        "id": "6KeAf7y5je1L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split into features and labels\n",
        "X_train = train_df[['dx', 'dv', 'v']].values\n",
        "y_train = train_df['v_next'].values\n",
        "X_validation = validation_df[['dx', 'dv', 'v']].values\n",
        "y_validation = validation_df['v_next'].values\n"
      ],
      "metadata": {
        "id": "0ntI-2PYjhZM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import os\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "lKrn5FuXjuDp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert to PyTorch tensors\n",
        "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
        "y_train = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)\n",
        "X_validation = torch.tensor(X_validation, dtype=torch.float32)\n",
        "y_validation = torch.tensor(y_validation, dtype=torch.float32).view(-1, 1)\n"
      ],
      "metadata": {
        "id": "gHAdPEUMjjWH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DataLoader for batch processing\n",
        "batch_size = 64\n",
        "train_dataset = TensorDataset(X_train, y_train)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "validation_dataset = TensorDataset(X_validation, y_validation)\n",
        "validation_loader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "pbiga36BjmGv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class SymbolicLayer(nn.Module):\n",
        "    \"\"\"Simple symbolic layer applying a predefined function to a sliced input.\"\"\"\n",
        "    def __init__(self, function, slice_index):\n",
        "        super().__init__()\n",
        "        self.function = function\n",
        "        self.slice_index = slice_index  # Define which part of input this layer should take\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Apply function to a specific part of the input (e.g., one feature)\n",
        "        return self.function(x[:, self.slice_index:self.slice_index+1])\n",
        "\n",
        "class Square(nn.Module):\n",
        "    def forward(self, x):\n",
        "        return x ** 2\n",
        "\n",
        "class Identity(nn.Module):\n",
        "    def forward(self, x):\n",
        "        return x\n",
        "\n",
        "class SymbolicNet(nn.Module):\n",
        "    \"\"\"A simple symbolic regression network that ensures a fixed number of outputs.\"\"\"\n",
        "    def __init__(self, funcs, input_features):\n",
        "        super().__init__()\n",
        "        self.hidden_layers = nn.ModuleList()\n",
        "        # Create layers and assign each to process a specific input feature\n",
        "        assert len(funcs) == 5  # Ensure there are exactly 5 functions\n",
        "        for i in range(5):\n",
        "            # Assign each function to a specific input feature\n",
        "            # Assume input_features is at least as large as len(funcs)\n",
        "            self.hidden_layers.append(SymbolicLayer(funcs[i % len(funcs)], i % input_features))\n",
        "\n",
        "        self.output_weight = nn.Parameter(torch.randn(5, 1))  # This matches the 5 symbolic layers\n",
        "\n",
        "    def forward(self, x):\n",
        "        outputs = []\n",
        "        for layer in self.hidden_layers:\n",
        "            outputs.append(layer(x))\n",
        "        x = torch.cat(outputs, dim=1)  # Concatenate all layer outputs\n",
        "        x = torch.matmul(x, self.output_weight)  # Multiply by the output weight\n",
        "        return x\n",
        "\n",
        "# Initialize the network with 5 instances of functions\n",
        "funcs = [Square(), Identity(), Square(), Identity(), Square()]  # Adjusted list to have exactly 5 elements\n",
        "net = SymbolicNet(funcs=funcs, input_features=3)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "net.to(device)\n",
        "\n",
        "# Assume train_loader and validation_loader are defined correctly\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(net.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 100\n",
        "for epoch in range(num_epochs):\n",
        "    net.train()\n",
        "    for inputs, targets in train_loader:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    net.eval()\n",
        "    validation_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in validation_loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = net(inputs)\n",
        "            validation_loss += criterion(outputs, targets).item()\n",
        "\n",
        "    print(f'Epoch {epoch+1}, Loss: {loss.item()}, Validation Loss: {validation_loss / len(validation_loader)}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LmBtqLMrm483",
        "outputId": "1a23b2fe-b581-4440-b1b8-c1971080b7b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 3990.19384765625, Validation Loss: 13197.075858200653\n",
            "Epoch 2, Loss: 21285.33203125, Validation Loss: 5147.203819564626\n",
            "Epoch 3, Loss: 1891.1995849609375, Validation Loss: 2810.648464540892\n",
            "Epoch 4, Loss: 1795.120361328125, Validation Loss: 1600.586901314651\n",
            "Epoch 5, Loss: 1502.567626953125, Validation Loss: 862.8171315253535\n",
            "Epoch 6, Loss: 246.92587280273438, Validation Loss: 435.2807819028444\n",
            "Epoch 7, Loss: 128.31495666503906, Validation Loss: 220.5752923217001\n",
            "Epoch 8, Loss: 68.9952621459961, Validation Loss: 125.7171401977539\n",
            "Epoch 9, Loss: 114.23883819580078, Validation Loss: 89.70983483519736\n",
            "Epoch 10, Loss: 98.37596130371094, Validation Loss: 77.21655041658425\n",
            "Epoch 11, Loss: 104.00625610351562, Validation Loss: 72.09335853480086\n",
            "Epoch 12, Loss: 64.01806640625, Validation Loss: 68.31754923470413\n",
            "Epoch 13, Loss: 43.97951126098633, Validation Loss: 63.95114565499221\n",
            "Epoch 14, Loss: 45.623294830322266, Validation Loss: 59.56732675093639\n",
            "Epoch 15, Loss: 36.84711837768555, Validation Loss: 54.44516119172302\n",
            "Epoch 16, Loss: 45.982383728027344, Validation Loss: 49.262654678731025\n",
            "Epoch 17, Loss: 34.55714797973633, Validation Loss: 44.00608321081234\n",
            "Epoch 18, Loss: 45.29624557495117, Validation Loss: 38.62001271791096\n",
            "Epoch 19, Loss: 33.32932662963867, Validation Loss: 33.437194993224324\n",
            "Epoch 20, Loss: 20.714757919311523, Validation Loss: 28.239382731763623\n",
            "Epoch 21, Loss: 23.539073944091797, Validation Loss: 23.370857938935487\n",
            "Epoch 22, Loss: 19.245275497436523, Validation Loss: 19.069300458401063\n",
            "Epoch 23, Loss: 10.737465858459473, Validation Loss: 15.158865928649902\n",
            "Epoch 24, Loss: 13.445423126220703, Validation Loss: 11.680292884005777\n",
            "Epoch 25, Loss: 6.720376491546631, Validation Loss: 8.969067428685442\n",
            "Epoch 26, Loss: 7.116861820220947, Validation Loss: 6.53898848762995\n",
            "Epoch 27, Loss: 5.532114028930664, Validation Loss: 4.771135936809491\n",
            "Epoch 28, Loss: 3.347777843475342, Validation Loss: 3.4830501381354995\n",
            "Epoch 29, Loss: 3.1184701919555664, Validation Loss: 2.6259270574473126\n",
            "Epoch 30, Loss: 1.0923166275024414, Validation Loss: 2.082020139392418\n",
            "Epoch 31, Loss: 1.6844185590744019, Validation Loss: 1.773477969290335\n",
            "Epoch 32, Loss: 1.8291336297988892, Validation Loss: 1.5782352225689948\n",
            "Epoch 33, Loss: 1.1032648086547852, Validation Loss: 1.5387429466730431\n",
            "Epoch 34, Loss: 0.7894582152366638, Validation Loss: 1.4920430273949346\n",
            "Epoch 35, Loss: 2.433164596557617, Validation Loss: 1.4921483163592182\n",
            "Epoch 36, Loss: 0.8078007698059082, Validation Loss: 1.55667644739151\n",
            "Epoch 37, Loss: 0.7684317827224731, Validation Loss: 1.6108250180377235\n",
            "Epoch 38, Loss: 1.2682546377182007, Validation Loss: 1.7055360025997404\n",
            "Epoch 39, Loss: 1.5433259010314941, Validation Loss: 1.4969473443453825\n",
            "Epoch 40, Loss: 0.8116101622581482, Validation Loss: 1.6358171844784217\n",
            "Epoch 41, Loss: 0.9755828976631165, Validation Loss: 1.70574737349643\n",
            "Epoch 42, Loss: 2.2166218757629395, Validation Loss: 1.6841582074950012\n",
            "Epoch 43, Loss: 1.387425422668457, Validation Loss: 1.4842304542094846\n",
            "Epoch 44, Loss: 1.5722365379333496, Validation Loss: 1.5089162218419812\n",
            "Epoch 45, Loss: 1.4226329326629639, Validation Loss: 1.7373455566695974\n",
            "Epoch 46, Loss: 1.4138740301132202, Validation Loss: 1.5991065871866443\n",
            "Epoch 47, Loss: 1.7521601915359497, Validation Loss: 1.7330942749977112\n",
            "Epoch 48, Loss: 1.7788180112838745, Validation Loss: 1.5057776547685455\n",
            "Epoch 49, Loss: 1.2116612195968628, Validation Loss: 1.511783337291283\n",
            "Epoch 50, Loss: 1.064020037651062, Validation Loss: 1.4900811872904813\n",
            "Epoch 51, Loss: 0.9957483410835266, Validation Loss: 1.5943065813825101\n",
            "Epoch 52, Loss: 2.325014352798462, Validation Loss: 1.667334825177736\n",
            "Epoch 53, Loss: 2.115626335144043, Validation Loss: 1.5965978367419182\n",
            "Epoch 54, Loss: 2.5713183879852295, Validation Loss: 1.586575674105294\n",
            "Epoch 55, Loss: 2.061802625656128, Validation Loss: 1.503356011607979\n",
            "Epoch 56, Loss: 1.5925469398498535, Validation Loss: 1.6905036432833611\n",
            "Epoch 57, Loss: 0.7556506395339966, Validation Loss: 1.4992070265963107\n",
            "Epoch 58, Loss: 1.8338778018951416, Validation Loss: 1.590187655974038\n",
            "Epoch 59, Loss: 2.0826821327209473, Validation Loss: 1.4964004166518585\n",
            "Epoch 60, Loss: 1.091751217842102, Validation Loss: 1.5439934564542166\n",
            "Epoch 61, Loss: 1.2677059173583984, Validation Loss: 1.7406988332543192\n",
            "Epoch 62, Loss: 1.165615439414978, Validation Loss: 1.5464520409137388\n",
            "Epoch 63, Loss: 1.0654072761535645, Validation Loss: 1.6056591061097156\n",
            "Epoch 64, Loss: 1.8949371576309204, Validation Loss: 1.5201766385307796\n",
            "Epoch 65, Loss: 1.0439761877059937, Validation Loss: 1.618266995194592\n",
            "Epoch 66, Loss: 0.8976360559463501, Validation Loss: 1.8154101832003533\n",
            "Epoch 67, Loss: 1.0288126468658447, Validation Loss: 1.5546172057526022\n",
            "Epoch 68, Loss: 0.8783904314041138, Validation Loss: 1.5162833876247648\n",
            "Epoch 69, Loss: 3.9064693450927734, Validation Loss: 1.616477919530265\n",
            "Epoch 70, Loss: 1.3142513036727905, Validation Loss: 1.5656128314476978\n",
            "Epoch 71, Loss: 1.077194333076477, Validation Loss: 1.631936464883104\n",
            "Epoch 72, Loss: 5.748496055603027, Validation Loss: 1.5111231645451317\n",
            "Epoch 73, Loss: 0.9782028198242188, Validation Loss: 1.5685371258590795\n",
            "Epoch 74, Loss: 2.0299508571624756, Validation Loss: 1.6963120185876195\n",
            "Epoch 75, Loss: 1.2843737602233887, Validation Loss: 1.597082093546662\n",
            "Epoch 76, Loss: 1.1484065055847168, Validation Loss: 1.5905368863781797\n",
            "Epoch 77, Loss: 1.1731066703796387, Validation Loss: 1.6167444579208954\n",
            "Epoch 78, Loss: 1.2148207426071167, Validation Loss: 1.5288222688662856\n",
            "Epoch 79, Loss: 2.135514497756958, Validation Loss: 1.5041149198254453\n",
            "Epoch 80, Loss: 0.7708114981651306, Validation Loss: 1.585305712645567\n",
            "Epoch 81, Loss: 1.6591942310333252, Validation Loss: 1.804989707620838\n",
            "Epoch 82, Loss: 1.9019352197647095, Validation Loss: 1.5160136879244936\n",
            "Epoch 83, Loss: 2.4012396335601807, Validation Loss: 1.5819338763816446\n",
            "Epoch 84, Loss: 0.6759140491485596, Validation Loss: 1.7830055793629418\n",
            "Epoch 85, Loss: 6.752551078796387, Validation Loss: 1.50250913976114\n",
            "Epoch 86, Loss: 1.7036911249160767, Validation Loss: 1.6573582329327547\n",
            "Epoch 87, Loss: 0.7259886860847473, Validation Loss: 1.73712502702882\n",
            "Epoch 88, Loss: 0.7104066014289856, Validation Loss: 1.4941442216498941\n",
            "Epoch 89, Loss: 1.1454098224639893, Validation Loss: 1.5238261864155154\n",
            "Epoch 90, Loss: 1.5692906379699707, Validation Loss: 1.627308908142621\n",
            "Epoch 91, Loss: 0.9486295580863953, Validation Loss: 1.990897878061367\n",
            "Epoch 92, Loss: 2.03340744972229, Validation Loss: 1.7408536883849133\n",
            "Epoch 93, Loss: 1.2069882154464722, Validation Loss: 1.516283193721047\n",
            "Epoch 94, Loss: 2.6081762313842773, Validation Loss: 1.5857827859588816\n",
            "Epoch 95, Loss: 7.006474494934082, Validation Loss: 1.5073799770089644\n",
            "Epoch 96, Loss: 2.415004253387451, Validation Loss: 1.511034823671172\n",
            "Epoch 97, Loss: 1.7544983625411987, Validation Loss: 1.4887585096721407\n",
            "Epoch 98, Loss: 0.9525920748710632, Validation Loss: 1.5386182718639132\n",
            "Epoch 99, Loss: 1.2903560400009155, Validation Loss: 1.6589372716372526\n",
            "Epoch 100, Loss: 1.7631031274795532, Validation Loss: 1.5009155167809016\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve output weights from the trained model\n",
        "output_weights = net.output_weight.detach().cpu().numpy().flatten()  # Flatten to make indexing easier\n",
        "\n",
        "# Define the function for each symbolic transformation\n",
        "# Assuming funcs are applied cyclically to input features dx, dv, v\n",
        "input_features = ['dx', 'dv', 'v']\n",
        "symbolic_equations = []\n",
        "\n",
        "for i, layer in enumerate(net.hidden_layers):\n",
        "    func_type = type(layer.function).__name__\n",
        "    feature = input_features[i % len(input_features)]\n",
        "\n",
        "    # Depending on the function type, format the symbolic expression\n",
        "    if func_type == 'Square':\n",
        "        expr = f\"({feature}^2)\"\n",
        "    elif func_type == 'Identity':\n",
        "        expr = f\"({feature})\"\n",
        "    else:\n",
        "        expr = f\"({feature})\"  # Default case, add more cases if you have more function types\n",
        "\n",
        "    # Multiply by the corresponding weight and add to the list\n",
        "    weight = output_weights[i]\n",
        "    symbolic_equations.append(f\"{weight:.3f} * {expr}\")\n",
        "\n",
        "# Combine all parts into a single equation string\n",
        "final_equation = \" + \".join(symbolic_equations) + \" + b\"\n",
        "print(\"Symbolic Equation:\", final_equation)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XwPN6dlnoI9P",
        "outputId": "3021b55d-b041-44a5-e615-926174ebb8f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Symbolic Equation: -0.007 * (dx^2) + 0.222 * (dv) + 0.056 * (v^2) + 0.390 * (dx) + -0.030 * (dv^2) + b\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class Cubic(nn.Module):\n",
        "    def forward(self, x):\n",
        "        return x ** 3\n",
        "\n",
        "class Sine(nn.Module):\n",
        "    def forward(self, x):\n",
        "        return torch.sin(x)\n",
        "\n",
        "class Cosine(nn.Module):\n",
        "    def forward(self, x):\n",
        "        return torch.cos(x)\n",
        "\n",
        "class Exponential(nn.Module):\n",
        "    def forward(self, x):\n",
        "        return torch.exp(x)\n",
        "\n",
        "class Logarithm(nn.Module):\n",
        "    def forward(self, x):\n",
        "        # To avoid issues with log(0) and negative numbers, apply a small shift; adjust as needed based on your data range\n",
        "        return torch.log(x + 1)\n",
        "\n",
        "class SymbolicNet(nn.Module):\n",
        "    \"\"\"A simple symbolic regression network with diverse symbolic functions.\"\"\"\n",
        "    def __init__(self, funcs, input_features):\n",
        "        super().__init__()\n",
        "        self.hidden_layers = nn.ModuleList()\n",
        "        # Assume that len(funcs) will correctly distribute across the number of outputs\n",
        "        for i in range(len(funcs)):\n",
        "            # Assign each function to a specific input feature, cycling through input features\n",
        "            self.hidden_layers.append(SymbolicLayer(funcs[i], i % input_features))\n",
        "\n",
        "        # Adjust the output weights to match the number of functions\n",
        "        self.output_weight = nn.Parameter(torch.randn(len(funcs), 1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        outputs = []\n",
        "        for layer in self.hidden_layers:\n",
        "            outputs.append(layer(x))\n",
        "        x = torch.cat(outputs, dim=1)\n",
        "        x = torch.matmul(x, self.output_weight)\n",
        "        return x\n",
        "\n",
        "# Define a richer set of functions\n",
        "funcs = [\n",
        "    Square(), Identity(), Cubic(), Sine(), Cosine(),\n",
        "    Exponential(), Logarithm(), Square(), Identity(), Square()\n",
        "]  # Extended list with more diverse functions\n",
        "\n",
        "net = SymbolicNet(funcs=funcs, input_features=3)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "net.to(device)\n",
        "\n",
        "# Assume train_loader and validation_loader are defined correctly\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(net.parameters(), lr=0.01)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 100\n",
        "for epoch in range(num_epochs):\n",
        "    net.train()\n",
        "    for inputs, targets in train_loader:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    net.eval()\n",
        "    validation_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in validation_loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = net(inputs)\n",
        "            validation_loss += criterion(outputs, targets).item()\n",
        "\n",
        "    print(f'Epoch {epoch+1}, Loss: {loss.item()}, Validation Loss: {validation_loss / len(validation_loader)}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i5cjgEMBj2SF",
        "outputId": "28714fe5-bbe7-4774-e326-b1feecfff3e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 59156596.0, Validation Loss: 2769026488.464794\n",
            "Epoch 2, Loss: 2415624.75, Validation Loss: 2842422.2492088606\n",
            "Epoch 3, Loss: 2739858.25, Validation Loss: 2691829.9485759493\n",
            "Epoch 4, Loss: 1067607.375, Validation Loss: 2492839.128164557\n",
            "Epoch 5, Loss: 4419076.0, Validation Loss: 2298383.6004746836\n",
            "Epoch 6, Loss: 1176516.125, Validation Loss: 2057017.9335443038\n",
            "Epoch 7, Loss: 1218706.375, Validation Loss: 1818321.05221519\n",
            "Epoch 8, Loss: 1173187.25, Validation Loss: 1556470.7954905063\n",
            "Epoch 9, Loss: 849842.8125, Validation Loss: 1348398.7448575948\n",
            "Epoch 10, Loss: 1651477.25, Validation Loss: 1080821.4909018988\n",
            "Epoch 11, Loss: 522408.5625, Validation Loss: 885257.3433544304\n",
            "Epoch 12, Loss: 476060.78125, Validation Loss: 718090.163568038\n",
            "Epoch 13, Loss: 636573.3125, Validation Loss: 585781.2480221519\n",
            "Epoch 14, Loss: 339830.625, Validation Loss: 437208.51720727846\n",
            "Epoch 15, Loss: 285594.84375, Validation Loss: 344741.0803006329\n",
            "Epoch 16, Loss: 664282.75, Validation Loss: 339498.6816653481\n",
            "Epoch 17, Loss: 187832.53125, Validation Loss: 230962.97379351265\n",
            "Epoch 18, Loss: 103366.6328125, Validation Loss: 263511.9864022943\n",
            "Epoch 19, Loss: 225633.046875, Validation Loss: 173462.04197982594\n",
            "Epoch 20, Loss: 107336.140625, Validation Loss: 152680.23284216772\n",
            "Epoch 21, Loss: 289125.28125, Validation Loss: 134693.72725474683\n",
            "Epoch 22, Loss: 94541.8515625, Validation Loss: 170041.21739022943\n",
            "Epoch 23, Loss: 375932.40625, Validation Loss: 247644.98617978639\n",
            "Epoch 24, Loss: 54299.25, Validation Loss: 65706.78441455697\n",
            "Epoch 25, Loss: 403064.21875, Validation Loss: 56585751.970727846\n",
            "Epoch 26, Loss: 96069.7421875, Validation Loss: 139548846.20436114\n",
            "Epoch 27, Loss: 11920220.0, Validation Loss: 417589038.87350386\n",
            "Epoch 28, Loss: 2073036.75, Validation Loss: 113712196.80955794\n",
            "Epoch 29, Loss: 128954.8125, Validation Loss: 91691.26369659811\n",
            "Epoch 30, Loss: 215259.15625, Validation Loss: 588250598.9046677\n",
            "Epoch 31, Loss: 393472.53125, Validation Loss: 9765256.210838608\n",
            "Epoch 32, Loss: 39735.703125, Validation Loss: 38424.13144654866\n",
            "Epoch 33, Loss: 31438.9453125, Validation Loss: 34242.36797863924\n",
            "Epoch 34, Loss: 11129.51171875, Validation Loss: 27875.81222186511\n",
            "Epoch 35, Loss: 16248.935546875, Validation Loss: 23524.955900168115\n",
            "Epoch 36, Loss: 15992.619140625, Validation Loss: 20997.007809409613\n",
            "Epoch 37, Loss: 11978.99609375, Validation Loss: 16181.583520816852\n",
            "Epoch 38, Loss: 5054.26123046875, Validation Loss: 40045.47591351859\n",
            "Epoch 39, Loss: 5374.52197265625, Validation Loss: 17870628.567382812\n",
            "Epoch 40, Loss: 33922.234375, Validation Loss: 800923.8606791436\n",
            "Epoch 41, Loss: 4741.29833984375, Validation Loss: 15316.278932209256\n",
            "Epoch 42, Loss: 2712.975830078125, Validation Loss: 36455.168982397154\n",
            "Epoch 43, Loss: 2596711.0, Validation Loss: 260885284.56269625\n",
            "Epoch 44, Loss: 2323.18408203125, Validation Loss: 4682.920636527146\n",
            "Epoch 45, Loss: 1864.514892578125, Validation Loss: 3025.0825697500495\n",
            "Epoch 46, Loss: 154257.515625, Validation Loss: 776307.0582638511\n",
            "Epoch 47, Loss: 689771712.0, Validation Loss: 225265.77234968354\n",
            "Epoch 48, Loss: 71197.953125, Validation Loss: 87325.26305379746\n",
            "Epoch 49, Loss: 18176.43359375, Validation Loss: 31884.420311263846\n",
            "Epoch 50, Loss: 7669.412109375, Validation Loss: 35876.34142911887\n",
            "Epoch 51, Loss: 3260.08544921875, Validation Loss: 6392.492499629154\n",
            "Epoch 52, Loss: 6612.0986328125, Validation Loss: 3786.5211482953423\n",
            "Epoch 53, Loss: 2706.693603515625, Validation Loss: 9549.67122138301\n",
            "Epoch 54, Loss: 4144.869140625, Validation Loss: 134340.53771624988\n",
            "Epoch 55, Loss: 1059.1485595703125, Validation Loss: 230978.37691294996\n",
            "Epoch 56, Loss: 3334471.25, Validation Loss: 152444829.64307258\n",
            "Epoch 57, Loss: 82671.8515625, Validation Loss: 520487.4730023734\n",
            "Epoch 58, Loss: 37756.40234375, Validation Loss: 36113.15555775316\n",
            "Epoch 59, Loss: 5081.96875, Validation Loss: 8420.68078149723\n",
            "Epoch 60, Loss: 1056.60302734375, Validation Loss: 1707.334254011323\n",
            "Epoch 61, Loss: 7707.70458984375, Validation Loss: 58948.41081237793\n",
            "Epoch 62, Loss: 5067307.5, Validation Loss: 337083.2767998418\n",
            "Epoch 63, Loss: 507436.0625, Validation Loss: 157758.7458959652\n",
            "Epoch 64, Loss: 33431.90625, Validation Loss: 80919.87082179589\n",
            "Epoch 65, Loss: 59444.52734375, Validation Loss: 45742.67539804193\n",
            "Epoch 66, Loss: 33732.62890625, Validation Loss: 34713.010445510285\n",
            "Epoch 67, Loss: 12579.21875, Validation Loss: 23247.560491248023\n",
            "Epoch 68, Loss: 6040.4228515625, Validation Loss: 16673.2768431072\n",
            "Epoch 69, Loss: 25221.345703125, Validation Loss: 14021.363840610165\n",
            "Epoch 70, Loss: 4757.53076171875, Validation Loss: 11184.526252843156\n",
            "Epoch 71, Loss: 61161.9140625, Validation Loss: 340247.164309731\n",
            "Epoch 72, Loss: 223104176.0, Validation Loss: 838025650.369116\n",
            "Epoch 73, Loss: 727974.25, Validation Loss: 47817.520421281646\n",
            "Epoch 74, Loss: 9248.1044921875, Validation Loss: 2351529.2429817296\n",
            "Epoch 75, Loss: 2609.790283203125, Validation Loss: 31175.95762692222\n",
            "Epoch 76, Loss: 8076626.0, Validation Loss: 2364512.236080894\n",
            "Epoch 77, Loss: 23756.416015625, Validation Loss: 113220.81683890427\n",
            "Epoch 78, Loss: 6474.41357421875, Validation Loss: 7826.762927091574\n",
            "Epoch 79, Loss: 4535.7568359375, Validation Loss: 29889.84230215338\n",
            "Epoch 80, Loss: 189565.875, Validation Loss: 64377968.87887457\n",
            "Epoch 81, Loss: 17412.39453125, Validation Loss: 231939.75930206684\n",
            "Epoch 82, Loss: 1813.4898681640625, Validation Loss: 5172.453590103342\n",
            "Epoch 83, Loss: 911.19287109375, Validation Loss: 1501.5257270909563\n",
            "Epoch 84, Loss: 363.4901123046875, Validation Loss: 789.5923137423358\n",
            "Epoch 85, Loss: 264.02264404296875, Validation Loss: 2404.9548258721074\n",
            "Epoch 86, Loss: 48995.59765625, Validation Loss: 466.9740556161615\n",
            "Epoch 87, Loss: 162975888.0, Validation Loss: 19891129737.780655\n",
            "Epoch 88, Loss: 353656.8125, Validation Loss: 334776.8846914557\n",
            "Epoch 89, Loss: 84225.859375, Validation Loss: 141370.06929885285\n",
            "Epoch 90, Loss: 37873.125, Validation Loss: 19791.71363231804\n",
            "Epoch 91, Loss: 12599.1171875, Validation Loss: 95104.33124011075\n",
            "Epoch 92, Loss: 6508.20458984375, Validation Loss: 9075.281419971321\n",
            "Epoch 93, Loss: 320700.8125, Validation Loss: 46506110.50296677\n",
            "Epoch 94, Loss: 86371.421875, Validation Loss: 89952.23012262658\n",
            "Epoch 95, Loss: 29693.609375, Validation Loss: 70423.91093502768\n",
            "Epoch 96, Loss: 19785.1328125, Validation Loss: 17671.988812796677\n",
            "Epoch 97, Loss: 16268.8486328125, Validation Loss: 18595.5419921875\n",
            "Epoch 98, Loss: 7616.23681640625, Validation Loss: 10522.53486575356\n",
            "Epoch 99, Loss: 2569.436279296875, Validation Loss: 6950.099155088014\n",
            "Epoch 100, Loss: 3830.876708984375, Validation Loss: 21619.87614653382\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve output weights from the trained model\n",
        "output_weights = net.output_weight.detach().cpu().numpy().flatten()  # Flatten to make indexing easier\n",
        "\n",
        "# Define the function for each symbolic transformation\n",
        "# Assuming funcs are applied cyclically to input features dx, dv, v\n",
        "input_features = ['dx', 'dv', 'v']\n",
        "symbolic_equations = []\n",
        "\n",
        "for i, layer in enumerate(net.hidden_layers):\n",
        "    func_type = type(layer.function).__name__\n",
        "    feature = input_features[i % len(input_features)]\n",
        "\n",
        "    # Depending on the function type, format the symbolic expression\n",
        "    if func_type == 'Square':\n",
        "        expr = f\"({feature}^2)\"\n",
        "    elif func_type == 'Identity':\n",
        "        expr = f\"({feature})\"\n",
        "    elif func_type == 'Cubic':\n",
        "        expr = f\"({feature}^3)\"\n",
        "    elif func_type == 'Sine':\n",
        "        expr = f\"sin({feature})\"\n",
        "    elif func_type == 'Cosine':\n",
        "        expr = f\"cos({feature})\"\n",
        "    elif func_type == 'Exponential':\n",
        "        expr = f\"exp({feature})\"\n",
        "    elif func_type == 'Logarithm':\n",
        "        expr = f\"log({feature} + 1)\"  # Adding 1 inside the log for numerical stability\n",
        "    else:\n",
        "        expr = f\"({feature})\"  # Default case if new types are added later\n",
        "\n",
        "    # Multiply by the corresponding weight and add to the list\n",
        "    weight = output_weights[i]\n",
        "    if weight != 0:  # Only include terms with non-zero weights to simplify the equation\n",
        "        symbolic_equations.append(f\"{weight:.3f} * {expr}\")\n",
        "\n",
        "# Combine all parts into a single equation string\n",
        "final_equation = \" + \".join(symbolic_equations) + \" + b\"  # Assuming 'b' is the bias term if it exists\n",
        "print(\"Symbolic Equation:\", final_equation)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dQ6SLBYoqtZY",
        "outputId": "92ad7a0f-d598-4a04-cb33-2557d5a9cc3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Symbolic Equation: -0.754 * (dx^2) + 0.083 * (dv) + -0.074 * (v^3) + 0.223 * sin(dx) + -4.574 * cos(dv) + -0.000 * exp(v) + -1.038 * log(dx + 1) + -0.372 * (dv^2) + -2.015 * (v) + 0.933 * (dx^2) + b\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class SymbolicLayer(nn.Module):\n",
        "    \"\"\"Simple symbolic layer applying a predefined function to a sliced input.\"\"\"\n",
        "    def __init__(self, function, slice_index):\n",
        "        super().__init__()\n",
        "        self.function = function\n",
        "        self.slice_index = slice_index\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.function(x[:, self.slice_index:self.slice_index+1])\n",
        "\n",
        "class Square(nn.Module):\n",
        "    def forward(self, x):\n",
        "        return x ** 2\n",
        "\n",
        "class Identity(nn.Module):\n",
        "    def forward(self, x):\n",
        "        return x\n",
        "\n",
        "class SymbolicNet(nn.Module):\n",
        "    \"\"\"A simple symbolic regression network that ensures a fixed number of outputs.\"\"\"\n",
        "    def __init__(self, funcs, input_features):\n",
        "        super().__init__()\n",
        "        self.hidden_layers = nn.ModuleList()\n",
        "        for i in range(5):\n",
        "            self.hidden_layers.append(SymbolicLayer(funcs[i % len(funcs)], i % input_features))\n",
        "\n",
        "        # Initialize the output weight using He initialization\n",
        "        self.output_weight = nn.Parameter(torch.randn(5, 1) * torch.sqrt(torch.tensor(2.0 / 5)))\n",
        "\n",
        "    def forward(self, x):\n",
        "        outputs = []\n",
        "        for layer in self.hidden_layers:\n",
        "            outputs.append(layer(x))\n",
        "        x = torch.cat(outputs, dim=1)\n",
        "        x = torch.matmul(x, self.output_weight)\n",
        "        return x\n",
        "\n",
        "# Initialize the network with 5 instances of functions\n",
        "funcs = [Square(), Identity(), Square(), Identity(), Square()]\n",
        "net = SymbolicNet(funcs=funcs, input_features=3)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "net.to(device)\n",
        "\n",
        "# Assume train_loader and validation_loader are defined correctly\n",
        "criterion = nn.L1Loss()\n",
        "optimizer = torch.optim.Adam(net.parameters(), lr=0.001)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)\n",
        "\n",
        "num_epochs = 100\n",
        "for epoch in range(num_epochs):\n",
        "    net.train()\n",
        "    for inputs, targets in train_loader:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    scheduler.step()\n",
        "\n",
        "    net.eval()\n",
        "    validation_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in validation_loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = net(inputs)\n",
        "            validation_loss += criterion(outputs, targets).item()\n",
        "\n",
        "    print(f'Epoch {epoch+1}, Loss: {loss.item()}, Validation Loss: {validation_loss / len(validation_loader)}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X8T8irnOtWNB",
        "outputId": "38e9689d-2103-4774-b0af-f21f62bbd3ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 2.7768242359161377, Validation Loss: 3.2524565262130545\n",
            "Epoch 2, Loss: 1.8897897005081177, Validation Loss: 1.9270253332355354\n",
            "Epoch 3, Loss: 1.3480901718139648, Validation Loss: 1.288722812374936\n",
            "Epoch 4, Loss: 0.8935331106185913, Validation Loss: 0.9640559327753284\n",
            "Epoch 5, Loss: 0.708904504776001, Validation Loss: 0.8772103356409676\n",
            "Epoch 6, Loss: 0.7686237096786499, Validation Loss: 0.8613172875174994\n",
            "Epoch 7, Loss: 0.9028732776641846, Validation Loss: 0.8577298635168921\n",
            "Epoch 8, Loss: 0.7291180491447449, Validation Loss: 0.858019873311248\n",
            "Epoch 9, Loss: 0.8118047118186951, Validation Loss: 0.8564581033549731\n",
            "Epoch 10, Loss: 0.8549567461013794, Validation Loss: 0.8569122251076035\n",
            "Epoch 11, Loss: 0.8033255338668823, Validation Loss: 0.8600225576871559\n",
            "Epoch 12, Loss: 1.0580894947052002, Validation Loss: 0.8555906875224053\n",
            "Epoch 13, Loss: 0.8287148475646973, Validation Loss: 0.907615606543384\n",
            "Epoch 14, Loss: 0.8828138113021851, Validation Loss: 0.8895916108843647\n",
            "Epoch 15, Loss: 0.6340140700340271, Validation Loss: 0.860731004914151\n",
            "Epoch 16, Loss: 0.9282943606376648, Validation Loss: 0.9028980271725715\n",
            "Epoch 17, Loss: 0.888281524181366, Validation Loss: 0.8594766772246059\n",
            "Epoch 18, Loss: 0.7912744283676147, Validation Loss: 0.8742662660683258\n",
            "Epoch 19, Loss: 0.8276032209396362, Validation Loss: 0.8673759095276459\n",
            "Epoch 20, Loss: 0.89518803358078, Validation Loss: 0.8603295140628573\n",
            "Epoch 21, Loss: 0.7058241963386536, Validation Loss: 0.857459610021567\n",
            "Epoch 22, Loss: 0.7905521392822266, Validation Loss: 0.8625842642180527\n",
            "Epoch 23, Loss: 0.7012385129928589, Validation Loss: 0.8569042260133768\n",
            "Epoch 24, Loss: 0.6388705968856812, Validation Loss: 0.8868981403640553\n",
            "Epoch 25, Loss: 0.9079487919807434, Validation Loss: 0.8607358615609664\n",
            "Epoch 26, Loss: 0.6341283321380615, Validation Loss: 0.8632624933991251\n",
            "Epoch 27, Loss: 1.1633867025375366, Validation Loss: 0.862013141565685\n",
            "Epoch 28, Loss: 0.9527491331100464, Validation Loss: 0.8580550762671458\n",
            "Epoch 29, Loss: 0.8751363158226013, Validation Loss: 0.8655654682388788\n",
            "Epoch 30, Loss: 0.6742660999298096, Validation Loss: 0.876045057290717\n",
            "Epoch 31, Loss: 0.6487494707107544, Validation Loss: 0.8555770788011672\n",
            "Epoch 32, Loss: 0.7829630374908447, Validation Loss: 0.859120215041728\n",
            "Epoch 33, Loss: 0.8709057569503784, Validation Loss: 0.8565114547934713\n",
            "Epoch 34, Loss: 0.8078789710998535, Validation Loss: 0.8558024122745176\n",
            "Epoch 35, Loss: 0.875883936882019, Validation Loss: 0.8549386481695538\n",
            "Epoch 36, Loss: 0.5842831134796143, Validation Loss: 0.8550840223891826\n",
            "Epoch 37, Loss: 0.7684539556503296, Validation Loss: 0.8573351653316353\n",
            "Epoch 38, Loss: 0.6676666140556335, Validation Loss: 0.8559521104716048\n",
            "Epoch 39, Loss: 0.6274648308753967, Validation Loss: 0.8550858490074738\n",
            "Epoch 40, Loss: 1.246918797492981, Validation Loss: 0.8550052024141143\n",
            "Epoch 41, Loss: 0.8389020562171936, Validation Loss: 0.8555740895150583\n",
            "Epoch 42, Loss: 0.6428366303443909, Validation Loss: 0.8549138549007947\n",
            "Epoch 43, Loss: 0.7169086933135986, Validation Loss: 0.8557723519168322\n",
            "Epoch 44, Loss: 0.7962404489517212, Validation Loss: 0.8549151164066943\n",
            "Epoch 45, Loss: 0.7308496236801147, Validation Loss: 0.8552946410601652\n",
            "Epoch 46, Loss: 1.000404953956604, Validation Loss: 0.8562809485423414\n",
            "Epoch 47, Loss: 1.08882474899292, Validation Loss: 0.8555329366575314\n",
            "Epoch 48, Loss: 0.8599228262901306, Validation Loss: 0.8549290616301042\n",
            "Epoch 49, Loss: 1.0464534759521484, Validation Loss: 0.855555015274241\n",
            "Epoch 50, Loss: 1.0487630367279053, Validation Loss: 0.8586372612397882\n",
            "Epoch 51, Loss: 0.6525627374649048, Validation Loss: 0.857756763319426\n",
            "Epoch 52, Loss: 0.6135984659194946, Validation Loss: 0.8556866102580782\n",
            "Epoch 53, Loss: 0.7389108538627625, Validation Loss: 0.8554018992411939\n",
            "Epoch 54, Loss: 1.030584454536438, Validation Loss: 0.8559161643438702\n",
            "Epoch 55, Loss: 0.7964456081390381, Validation Loss: 0.8549318238149716\n",
            "Epoch 56, Loss: 0.8942978978157043, Validation Loss: 0.8586877554277831\n",
            "Epoch 57, Loss: 0.7964394092559814, Validation Loss: 0.8549992434586151\n",
            "Epoch 58, Loss: 0.8707208037376404, Validation Loss: 0.8557525827914854\n",
            "Epoch 59, Loss: 0.7630720138549805, Validation Loss: 0.8549611923060839\n",
            "Epoch 60, Loss: 0.7786329388618469, Validation Loss: 0.8554904000668586\n",
            "Epoch 61, Loss: 0.6690298318862915, Validation Loss: 0.8549650077578388\n",
            "Epoch 62, Loss: 0.7412512302398682, Validation Loss: 0.8554047406474247\n",
            "Epoch 63, Loss: 0.9114366769790649, Validation Loss: 0.8550613364086875\n",
            "Epoch 64, Loss: 0.7842532992362976, Validation Loss: 0.855249926259246\n",
            "Epoch 65, Loss: 0.9618275165557861, Validation Loss: 0.8549177925797957\n",
            "Epoch 66, Loss: 1.1281943321228027, Validation Loss: 0.8549919724464417\n",
            "Epoch 67, Loss: 0.8448602557182312, Validation Loss: 0.8549685168869888\n",
            "Epoch 68, Loss: 0.7357838749885559, Validation Loss: 0.8555074054983598\n",
            "Epoch 69, Loss: 0.703713595867157, Validation Loss: 0.8551718751086465\n",
            "Epoch 70, Loss: 0.7365420460700989, Validation Loss: 0.8551336360883109\n",
            "Epoch 71, Loss: 0.8713124990463257, Validation Loss: 0.8550351948677739\n",
            "Epoch 72, Loss: 0.74541175365448, Validation Loss: 0.8549297195446642\n",
            "Epoch 73, Loss: 0.773766279220581, Validation Loss: 0.8550900069973136\n",
            "Epoch 74, Loss: 0.669339120388031, Validation Loss: 0.8550006158744232\n",
            "Epoch 75, Loss: 1.311371088027954, Validation Loss: 0.855257411546345\n",
            "Epoch 76, Loss: 0.6651058197021484, Validation Loss: 0.8549683991866776\n",
            "Epoch 77, Loss: 0.6689908504486084, Validation Loss: 0.8552692117570322\n",
            "Epoch 78, Loss: 0.8018487691879272, Validation Loss: 0.8550570131857184\n",
            "Epoch 79, Loss: 1.1449682712554932, Validation Loss: 0.8550801963745793\n",
            "Epoch 80, Loss: 0.7826446294784546, Validation Loss: 0.8549645301661913\n",
            "Epoch 81, Loss: 0.717595636844635, Validation Loss: 0.8551994132090218\n",
            "Epoch 82, Loss: 0.6938747763633728, Validation Loss: 0.855051789102675\n",
            "Epoch 83, Loss: 0.7786358594894409, Validation Loss: 0.8551186028915115\n",
            "Epoch 84, Loss: 0.708920419216156, Validation Loss: 0.8549949398523644\n",
            "Epoch 85, Loss: 0.7223485112190247, Validation Loss: 0.8549606709540645\n",
            "Epoch 86, Loss: 0.9695830345153809, Validation Loss: 0.8549660112284407\n",
            "Epoch 87, Loss: 0.8065683245658875, Validation Loss: 0.8550501895856254\n",
            "Epoch 88, Loss: 0.7792412638664246, Validation Loss: 0.8552697195282465\n",
            "Epoch 89, Loss: 0.9116140604019165, Validation Loss: 0.8550005223177657\n",
            "Epoch 90, Loss: 0.9060859084129333, Validation Loss: 0.8551511787161042\n",
            "Epoch 91, Loss: 0.6496977806091309, Validation Loss: 0.8549629502658602\n",
            "Epoch 92, Loss: 0.8150731325149536, Validation Loss: 0.8549261779724797\n",
            "Epoch 93, Loss: 0.9268473386764526, Validation Loss: 0.8549273685563968\n",
            "Epoch 94, Loss: 0.7691644430160522, Validation Loss: 0.8549388994144488\n",
            "Epoch 95, Loss: 0.9284600019454956, Validation Loss: 0.8549542759038224\n",
            "Epoch 96, Loss: 0.5985666513442993, Validation Loss: 0.8549650771708428\n",
            "Epoch 97, Loss: 0.8263161182403564, Validation Loss: 0.8549603540686113\n",
            "Epoch 98, Loss: 1.2934528589248657, Validation Loss: 0.8549552801289136\n",
            "Epoch 99, Loss: 1.0010809898376465, Validation Loss: 0.854958072493348\n",
            "Epoch 100, Loss: 0.9373149871826172, Validation Loss: 0.8549571497530877\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve output weights from the trained model\n",
        "output_weights = net.output_weight.detach().cpu().numpy().flatten()  # Flatten to make indexing easier\n",
        "\n",
        "# Define the function for each symbolic transformation\n",
        "# Assuming funcs are applied cyclically to input features dx, dv, v\n",
        "input_features = ['dx', 'dv', 'v']\n",
        "symbolic_equations = []\n",
        "\n",
        "for i, layer in enumerate(net.hidden_layers):\n",
        "    func_type = type(layer.function).__name__\n",
        "    feature = input_features[i % len(input_features)]\n",
        "\n",
        "    # Depending on the function type, format the symbolic expression\n",
        "    if func_type == 'Square':\n",
        "        expr = f\"({feature}^2)\"\n",
        "    elif func_type == 'Identity':\n",
        "        expr = f\"({feature})\"\n",
        "    else:\n",
        "        expr = f\"({feature})\"  # Default case, add more cases if you have more function types\n",
        "\n",
        "    # Multiply by the corresponding weight and add to the list\n",
        "    weight = output_weights[i]\n",
        "    symbolic_equations.append(f\"{weight:.3f} * {expr}\")\n",
        "\n",
        "# Combine all parts into a single equation string\n",
        "final_equation = \" + \".join(symbolic_equations) + \" + b\"\n",
        "print(\"Symbolic Equation:\", final_equation)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FWyuM7Vvt6z0",
        "outputId": "0bed0542-a9eb-4f3c-b306-d03f8cb0a65e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Symbolic Equation: -0.010 * (dx^2) + 0.227 * (dv) + 0.057 * (v^2) + 0.448 * (dx) + -0.038 * (dv^2) + b\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HzwYDqVG3W8G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scheduler = torch.optim.lr_scheduler.CyclicLR(\n",
        "    optimizer,\n",
        "    base_lr=0.0001,\n",
        "    max_lr=0.01,\n",
        "    step_size_up=10,\n",
        "    mode='exp_range',\n",
        "    gamma=0.85,\n",
        "    cycle_momentum=False  # Disable cycle_momentum\n",
        ")\n"
      ],
      "metadata": {
        "id": "riQ-twIbvXPV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "class InteractionLayer(nn.Module):\n",
        "    \"\"\" Layer to create interaction terms between features. \"\"\"\n",
        "    def __init__(self, slice_indices):\n",
        "        super().__init__()\n",
        "        self.slice_indices = slice_indices\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x[:, self.slice_indices[0]] * x[:, self.slice_indices[1]]\n",
        "\n",
        "class SymbolicLayer(nn.Module):\n",
        "    \"\"\" Simple symbolic layer applying a predefined function to a sliced input. \"\"\"\n",
        "    def __init__(self, function, slice_index):\n",
        "        super().__init__()\n",
        "        self.function = function\n",
        "        self.slice_index = slice_index\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.function(x[:, self.slice_index:self.slice_index+1])\n",
        "\n",
        "class Square(nn.Module):\n",
        "    def forward(self, x):\n",
        "        return x ** 2\n",
        "\n",
        "class Identity(nn.Module):\n",
        "    def forward(self, x):\n",
        "        return x\n",
        "class InteractionLayer(nn.Module):\n",
        "    \"\"\"Layer to create interaction terms between features.\"\"\"\n",
        "    def __init__(self, slice_indices):\n",
        "        super().__init__()\n",
        "        self.slice_indices = slice_indices\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x[:, self.slice_indices[0]:self.slice_indices[0]+1] * x[:, self.slice_indices[1]:self.slice_indices[1]+1]\n",
        "\n",
        "class SymbolicLayer(nn.Module):\n",
        "    \"\"\"Simple symbolic layer applying a predefined function to a sliced input.\"\"\"\n",
        "    def __init__(self, function, slice_index):\n",
        "        super().__init__()\n",
        "        self.function = function\n",
        "        self.slice_index = slice_index\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.function(x[:, self.slice_index:self.slice_index+1])\n",
        "class SymbolicNet(nn.Module):\n",
        "    def __init__(self, funcs, input_features):\n",
        "        super().__init__()\n",
        "        self.hidden_layers = nn.ModuleList()\n",
        "        for i in range(5):\n",
        "            self.hidden_layers.append(SymbolicLayer(funcs[i % len(funcs)], i % input_features))\n",
        "        self.hidden_layers.append(InteractionLayer([0, 1]))  # Interaction between the first and second feature\n",
        "        self.hidden_layers.append(InteractionLayer([1, 2]))  # Interaction between the second and third feature\n",
        "        self.output_weight = nn.Parameter(torch.randn(7, 1) * torch.sqrt(torch.tensor(2.0 / 7)))\n",
        "\n",
        "    def forward(self, x):\n",
        "        outputs = []\n",
        "        for layer in self.hidden_layers:\n",
        "            outputs.append(layer(x))\n",
        "        x = torch.cat(outputs, dim=1)  # Change dim to 1\n",
        "        x = torch.matmul(x, self.output_weight)\n",
        "        return x\n",
        "\n",
        "funcs = [Square(), Identity(), Square(), Identity(), Square()]\n",
        "net = SymbolicNet(funcs=funcs, input_features=3)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "net.to(device)\n",
        "\n",
        "criterion = nn.L1Loss()\n",
        "optimizer = torch.optim.AdamW(net.parameters(), lr=0.001, weight_decay=0.01)\n",
        "scheduler = torch.optim.lr_scheduler.CyclicLR(\n",
        "    optimizer,\n",
        "    base_lr=0.0001,\n",
        "    max_lr=0.01,\n",
        "    step_size_up=10,\n",
        "    mode='exp_range',\n",
        "    gamma=0.85,\n",
        "    cycle_momentum=False  # Disable cycle_momentum for AdamW\n",
        ")\n",
        "\n",
        "num_epochs = 100\n",
        "for epoch in range(num_epochs):\n",
        "    net.train()\n",
        "    for inputs, targets in train_loader:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    scheduler.step()\n",
        "\n",
        "    net.eval()\n",
        "    validation_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in validation_loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = net(inputs)\n",
        "            validation_loss += criterion(outputs, targets).item()\n",
        "\n",
        "    print(f'Epoch {epoch+1}, Loss: {loss.item()}, Validation Loss: {validation_loss / len(validation_loader)}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_lSPBDNTvaP4",
        "outputId": "7a816035-b58d-4534-b28a-2d6f0c12bcd9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 146.00149536132812, Validation Loss: 127.98545451103887\n",
            "Epoch 2, Loss: 48.239356994628906, Validation Loss: 38.98533720909795\n",
            "Epoch 3, Loss: 2.5769479274749756, Validation Loss: 2.742592715009858\n",
            "Epoch 4, Loss: 1.2530279159545898, Validation Loss: 1.291163305693035\n",
            "Epoch 5, Loss: 0.8635858297348022, Validation Loss: 0.9402038153213791\n",
            "Epoch 6, Loss: 0.92360919713974, Validation Loss: 0.8982871853852574\n",
            "Epoch 7, Loss: 1.012155294418335, Validation Loss: 0.8608705099624924\n",
            "Epoch 8, Loss: 0.7848567962646484, Validation Loss: 0.8331959564474565\n",
            "Epoch 9, Loss: 0.8587113618850708, Validation Loss: 0.90227145635629\n",
            "Epoch 10, Loss: 0.7860592007637024, Validation Loss: 0.8362398449378677\n",
            "Epoch 11, Loss: 0.7762719988822937, Validation Loss: 0.8636467735978621\n",
            "Epoch 12, Loss: 0.8336063623428345, Validation Loss: 0.843587412864347\n",
            "Epoch 13, Loss: 0.7671655416488647, Validation Loss: 0.8407607923580122\n",
            "Epoch 14, Loss: 0.8173310160636902, Validation Loss: 0.8328217335894138\n",
            "Epoch 15, Loss: 0.8646233081817627, Validation Loss: 0.8442150890072689\n",
            "Epoch 16, Loss: 0.731520414352417, Validation Loss: 0.8329413155966168\n",
            "Epoch 17, Loss: 0.9156360626220703, Validation Loss: 0.8318538582777675\n",
            "Epoch 18, Loss: 0.6642512083053589, Validation Loss: 0.8379003918623622\n",
            "Epoch 19, Loss: 1.0060091018676758, Validation Loss: 0.8332434100440785\n",
            "Epoch 20, Loss: 0.684309720993042, Validation Loss: 0.8368126304843758\n",
            "Epoch 21, Loss: 0.7930218577384949, Validation Loss: 0.8317458727691747\n",
            "Epoch 22, Loss: 0.7841094732284546, Validation Loss: 0.8328456652315357\n",
            "Epoch 23, Loss: 1.2262017726898193, Validation Loss: 0.8337142912647392\n",
            "Epoch 24, Loss: 0.7704594135284424, Validation Loss: 0.8319478050062928\n",
            "Epoch 25, Loss: 0.8100296854972839, Validation Loss: 0.8364638852167733\n",
            "Epoch 26, Loss: 0.9188526272773743, Validation Loss: 0.8336478471755981\n",
            "Epoch 27, Loss: 0.7479156851768494, Validation Loss: 0.8326780049106742\n",
            "Epoch 28, Loss: 0.8212416768074036, Validation Loss: 0.8437481235854233\n",
            "Epoch 29, Loss: 0.6747488975524902, Validation Loss: 0.8357955715324306\n",
            "Epoch 30, Loss: 0.8113200664520264, Validation Loss: 0.8323186741599554\n",
            "Epoch 31, Loss: 0.8040273189544678, Validation Loss: 0.8331074224242682\n",
            "Epoch 32, Loss: 0.6545971035957336, Validation Loss: 0.8335992852343789\n",
            "Epoch 33, Loss: 1.1961314678192139, Validation Loss: 0.8334861133672014\n",
            "Epoch 34, Loss: 0.8437605500221252, Validation Loss: 0.831916327717938\n",
            "Epoch 35, Loss: 0.7591791152954102, Validation Loss: 0.8324819803237915\n",
            "Epoch 36, Loss: 0.8844422101974487, Validation Loss: 0.8324059089527854\n",
            "Epoch 37, Loss: 0.8049330711364746, Validation Loss: 0.8320179159128214\n",
            "Epoch 38, Loss: 0.77220219373703, Validation Loss: 0.831898505174661\n",
            "Epoch 39, Loss: 0.6342929005622864, Validation Loss: 0.8317417472223693\n",
            "Epoch 40, Loss: 0.7052335143089294, Validation Loss: 0.8383542238911496\n",
            "Epoch 41, Loss: 0.794098973274231, Validation Loss: 0.8323755792424649\n",
            "Epoch 42, Loss: 0.6508139371871948, Validation Loss: 0.8317253740527962\n",
            "Epoch 43, Loss: 0.749479353427887, Validation Loss: 0.8321385617497601\n",
            "Epoch 44, Loss: 0.7705414295196533, Validation Loss: 0.8332690083527867\n",
            "Epoch 45, Loss: 0.7383730411529541, Validation Loss: 0.8485119372983522\n",
            "Epoch 46, Loss: 0.868881344795227, Validation Loss: 0.8328164317939855\n",
            "Epoch 47, Loss: 0.7699136734008789, Validation Loss: 0.8329536009438431\n",
            "Epoch 48, Loss: 0.9375691413879395, Validation Loss: 0.8323537005653864\n",
            "Epoch 49, Loss: 0.80869060754776, Validation Loss: 0.8339946021007586\n",
            "Epoch 50, Loss: 0.7602677941322327, Validation Loss: 0.833894219579576\n",
            "Epoch 51, Loss: 0.7605720162391663, Validation Loss: 0.8322392430486558\n",
            "Epoch 52, Loss: 0.7837460041046143, Validation Loss: 0.8365379828440992\n",
            "Epoch 53, Loss: 0.7821189165115356, Validation Loss: 0.832179039339476\n",
            "Epoch 54, Loss: 0.7657327055931091, Validation Loss: 0.8315901288503333\n",
            "Epoch 55, Loss: 0.7579524517059326, Validation Loss: 0.8319509044478212\n",
            "Epoch 56, Loss: 0.742976188659668, Validation Loss: 0.8348968104471134\n",
            "Epoch 57, Loss: 0.7995851039886475, Validation Loss: 0.8321588703348667\n",
            "Epoch 58, Loss: 0.8059883713722229, Validation Loss: 0.8327356056322025\n",
            "Epoch 59, Loss: 0.6661179065704346, Validation Loss: 0.8325706893884683\n",
            "Epoch 60, Loss: 0.7190003991127014, Validation Loss: 0.8331582546234131\n",
            "Epoch 61, Loss: 0.8756561279296875, Validation Loss: 0.847801919224896\n",
            "Epoch 62, Loss: 0.7999507784843445, Validation Loss: 0.8325563125972506\n",
            "Epoch 63, Loss: 0.6556614637374878, Validation Loss: 0.8316088809242731\n",
            "Epoch 64, Loss: 1.2273499965667725, Validation Loss: 0.8325321168839177\n",
            "Epoch 65, Loss: 1.2281360626220703, Validation Loss: 0.8323607135422623\n",
            "Epoch 66, Loss: 0.8574652671813965, Validation Loss: 0.8316792408122292\n",
            "Epoch 67, Loss: 0.7955694198608398, Validation Loss: 0.8319134312339976\n",
            "Epoch 68, Loss: 0.8602170944213867, Validation Loss: 0.8323540340496015\n",
            "Epoch 69, Loss: 0.6787009239196777, Validation Loss: 0.832169840607462\n",
            "Epoch 70, Loss: 0.6482881903648376, Validation Loss: 0.8322037533868717\n",
            "Epoch 71, Loss: 0.7919837236404419, Validation Loss: 0.8320390235019636\n",
            "Epoch 72, Loss: 0.7457706332206726, Validation Loss: 0.8334643380551399\n",
            "Epoch 73, Loss: 0.9022784233093262, Validation Loss: 0.8320673658877988\n",
            "Epoch 74, Loss: 0.713653028011322, Validation Loss: 0.8358617304246637\n",
            "Epoch 75, Loss: 0.8368489146232605, Validation Loss: 0.8316243434254127\n",
            "Epoch 76, Loss: 0.745171070098877, Validation Loss: 0.8316680381569681\n",
            "Epoch 77, Loss: 0.8420801758766174, Validation Loss: 0.8321853106534933\n",
            "Epoch 78, Loss: 0.7547447681427002, Validation Loss: 0.8351929233044009\n",
            "Epoch 79, Loss: 1.1027411222457886, Validation Loss: 0.8356230628641346\n",
            "Epoch 80, Loss: 0.7851616144180298, Validation Loss: 0.8319847674309453\n",
            "Epoch 81, Loss: 0.7781294584274292, Validation Loss: 0.8320967268340195\n",
            "Epoch 82, Loss: 0.7798985242843628, Validation Loss: 0.8335033640076842\n",
            "Epoch 83, Loss: 0.6589140892028809, Validation Loss: 0.8336356621754321\n",
            "Epoch 84, Loss: 1.4610053300857544, Validation Loss: 0.8329666279539277\n",
            "Epoch 85, Loss: 0.7964634895324707, Validation Loss: 0.8316678842411765\n",
            "Epoch 86, Loss: 0.697913646697998, Validation Loss: 0.8331590008132065\n",
            "Epoch 87, Loss: 1.06057608127594, Validation Loss: 0.8343888332572165\n",
            "Epoch 88, Loss: 1.00491201877594, Validation Loss: 0.8321218679222879\n",
            "Epoch 89, Loss: 0.6332623958587646, Validation Loss: 0.8433289346815664\n",
            "Epoch 90, Loss: 0.7090452909469604, Validation Loss: 0.832738446283944\n",
            "Epoch 91, Loss: 0.9454004764556885, Validation Loss: 0.8319873440114758\n",
            "Epoch 92, Loss: 0.704748809337616, Validation Loss: 0.8353699415544921\n",
            "Epoch 93, Loss: 1.0248982906341553, Validation Loss: 0.832045511354374\n",
            "Epoch 94, Loss: 0.7217624187469482, Validation Loss: 0.835675883142254\n",
            "Epoch 95, Loss: 0.8946526050567627, Validation Loss: 0.8339704003515123\n",
            "Epoch 96, Loss: 0.776857316493988, Validation Loss: 0.8324789359599729\n",
            "Epoch 97, Loss: 0.6956908106803894, Validation Loss: 0.831735502315473\n",
            "Epoch 98, Loss: 0.7480891942977905, Validation Loss: 0.8319899024842661\n",
            "Epoch 99, Loss: 1.6226361989974976, Validation Loss: 0.8316779928871348\n",
            "Epoch 100, Loss: 1.004384994506836, Validation Loss: 0.8330346059195602\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve output weights from the trained model\n",
        "output_weights = net.output_weight.detach().cpu().numpy().flatten()  # Flatten to make indexing easier\n",
        "\n",
        "# Define the function for each symbolic transformation\n",
        "# Assuming funcs are applied cyclically to input features dx, dv, v\n",
        "input_features = ['dx', 'dv', 'v']\n",
        "symbolic_equations = []\n",
        "\n",
        "for i, layer in enumerate(net.hidden_layers):\n",
        "    if isinstance(layer, SymbolicLayer):\n",
        "        func_type = type(layer.function).__name__\n",
        "        feature = input_features[layer.slice_index]\n",
        "        # Depending on the function type, format the symbolic expression\n",
        "        if func_type == 'Square':\n",
        "            expr = f\"({feature}^2)\"\n",
        "        elif func_type == 'Identity':\n",
        "            expr = f\"({feature})\"\n",
        "        else:\n",
        "            expr = f\"({feature})\"  # Default case, add more cases if you have more function types\n",
        "    elif isinstance(layer, InteractionLayer):\n",
        "        feature1 = input_features[layer.slice_indices[0]]\n",
        "        feature2 = input_features[layer.slice_indices[1]]\n",
        "        expr = f\"({feature1} * {feature2})\"\n",
        "    else:\n",
        "        continue  # Skip layers that are not SymbolicLayer or InteractionLayer\n",
        "\n",
        "    # Multiply by the corresponding weight and add to the list\n",
        "    weight = output_weights[len(symbolic_equations)]  # Use the current length of symbolic_equations as the index\n",
        "    symbolic_equations.append(f\"{weight:.3f} * {expr}\")\n",
        "\n",
        "# Combine all parts into a single equation string\n",
        "final_equation = \" + \".join(symbolic_equations) + \" + b\"\n",
        "print(\"Symbolic Equation:\", final_equation)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s1JJGD6N4I3f",
        "outputId": "ebd952d2-a9ae-4cb3-dceb-7cdf430160e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Symbolic Equation: -0.011 * (dx^2) + -0.210 * (dv) + 0.057 * (v^2) + 0.450 * (dx) + -0.005 * (dv^2) + -0.002 * (dx * dv) + 0.068 * (dv * v) + b\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cnwsq-WF8sCf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Methods for regularization to produce sparse networks.\n",
        "\n",
        "L2 regularization mostly penalizes the weight magnitudes without introducing sparsity.\n",
        "L1 regularization promotes sparsity.\n",
        "L1/2 promotes sparsity even more than L1. However, it can be difficult to train due to non-convexity and exploding\n",
        "gradients close to 0. Thus, we introduce a smoothed L1/2 regularization to remove the exploding gradients.\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class L12Smooth(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(L12Smooth, self).__init__()\n",
        "\n",
        "    def forward(self, input_tensor, a=0.05):\n",
        "        \"\"\"input: predictions\"\"\"\n",
        "        return l12_smooth(input_tensor, a)\n",
        "\n",
        "\n",
        "def l12_smooth(input_tensor, a=0.05):\n",
        "    \"\"\"Smoothed L1/2 norm\"\"\"\n",
        "    if type(input_tensor) == list:\n",
        "        return sum([l12_smooth(tensor) for tensor in input_tensor])\n",
        "\n",
        "    smooth_abs = torch.where(torch.abs(input_tensor) < a,\n",
        "                             torch.pow(input_tensor, 4) / (-8 * a ** 3) + torch.square(input_tensor) * 3 / 4 / a + 3 * a / 8,\n",
        "                             torch.abs(input_tensor))\n",
        "\n",
        "    return torch.sum(torch.sqrt(smooth_abs))"
      ],
      "metadata": {
        "id": "YshfGuUh5G9P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#regularization\n",
        "\n",
        "l12_smooth_reg = L12Smooth()"
      ],
      "metadata": {
        "id": "pGkZhT7x5PGJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "class InteractionLayer(nn.Module):\n",
        "    \"\"\" Layer to create interaction terms between features. \"\"\"\n",
        "    def __init__(self, slice_indices):\n",
        "        super().__init__()\n",
        "        self.slice_indices = slice_indices\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x[:, self.slice_indices[0]] * x[:, self.slice_indices[1]]\n",
        "\n",
        "class SymbolicLayer(nn.Module):\n",
        "    \"\"\" Simple symbolic layer applying a predefined function to a sliced input. \"\"\"\n",
        "    def __init__(self, function, slice_index):\n",
        "        super().__init__()\n",
        "        self.function = function\n",
        "        self.slice_index = slice_index\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.function(x[:, self.slice_index:self.slice_index+1])\n",
        "\n",
        "class Square(nn.Module):\n",
        "    def forward(self, x):\n",
        "        return x ** 2\n",
        "\n",
        "class Identity(nn.Module):\n",
        "    def forward(self, x):\n",
        "        return x\n",
        "class InteractionLayer(nn.Module):\n",
        "    \"\"\"Layer to create interaction terms between features.\"\"\"\n",
        "    def __init__(self, slice_indices):\n",
        "        super().__init__()\n",
        "        self.slice_indices = slice_indices\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x[:, self.slice_indices[0]:self.slice_indices[0]+1] * x[:, self.slice_indices[1]:self.slice_indices[1]+1]\n",
        "\n",
        "class SymbolicLayer(nn.Module):\n",
        "    \"\"\"Simple symbolic layer applying a predefined function to a sliced input.\"\"\"\n",
        "    def __init__(self, function, slice_index):\n",
        "        super().__init__()\n",
        "        self.function = function\n",
        "        self.slice_index = slice_index\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.function(x[:, self.slice_index:self.slice_index+1])\n",
        "class SymbolicNet(nn.Module):\n",
        "    def __init__(self, funcs, input_features):\n",
        "        super().__init__()\n",
        "        self.hidden_layers = nn.ModuleList()\n",
        "        for i in range(5):\n",
        "            self.hidden_layers.append(SymbolicLayer(funcs[i % len(funcs)], i % input_features))\n",
        "        self.hidden_layers.append(InteractionLayer([0, 1]))  # Interaction between the first and second feature\n",
        "        self.hidden_layers.append(InteractionLayer([1, 2]))  # Interaction between the second and third feature\n",
        "        self.output_weight = nn.Parameter(torch.randn(7, 1) * torch.sqrt(torch.tensor(2.0 / 7)))\n",
        "\n",
        "    def forward(self, x):\n",
        "        outputs = []\n",
        "        for layer in self.hidden_layers:\n",
        "            outputs.append(layer(x))\n",
        "        x = torch.cat(outputs, dim=1)  # Change dim to 1\n",
        "        x = torch.matmul(x, self.output_weight)\n",
        "        return x\n",
        "\n",
        "funcs = [Square(), Identity(), Square(), Identity(), Square()]\n",
        "net = SymbolicNet(funcs=funcs, input_features=3)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "net.to(device)\n",
        "\n",
        "criterion = nn.L1Loss()\n",
        "optimizer = torch.optim.AdamW(net.parameters(), lr=0.001, weight_decay=0.01)\n",
        "scheduler = torch.optim.lr_scheduler.CyclicLR(\n",
        "    optimizer,\n",
        "    base_lr=0.0001,\n",
        "    max_lr=0.01,\n",
        "    step_size_up=10,\n",
        "    mode='exp_range',\n",
        "    gamma=0.85,\n",
        "    cycle_momentum=False  # Disable cycle_momentum for AdamW\n",
        ")\n",
        "\n",
        "num_epochs = 100\n",
        "reg_lambda = 0.01  # Regularization strength\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    net.train()\n",
        "    for inputs, targets in train_loader:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = net(inputs)\n",
        "\n",
        "        # Calculate the regularization term for each parameter tensor\n",
        "        reg_term = 0\n",
        "        for param in net.parameters():\n",
        "            reg_term += l12_smooth_reg(param)\n",
        "\n",
        "        # Add the regularization term to the loss\n",
        "        loss = criterion(outputs, targets) + reg_lambda * reg_term\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    scheduler.step()\n",
        "\n",
        "    net.eval()\n",
        "    validation_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in validation_loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = net(inputs)\n",
        "            validation_loss += criterion(outputs, targets).item()\n",
        "    print(f'Epoch {epoch+1}, Loss: {loss.item()}, Validation Loss: {validation_loss / len(validation_loader)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1IwJgDuP5XLM",
        "outputId": "1aa48164-3098-4d07-d982-92e16d7b779e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 65.62969207763672, Validation Loss: 70.86356604853763\n",
            "Epoch 2, Loss: 2.521970748901367, Validation Loss: 3.1768368829654743\n",
            "Epoch 3, Loss: 0.715196430683136, Validation Loss: 1.0538983895808836\n",
            "Epoch 4, Loss: 0.8776630163192749, Validation Loss: 0.8699867008607599\n",
            "Epoch 5, Loss: 0.8916141986846924, Validation Loss: 0.884906184069718\n",
            "Epoch 6, Loss: 0.8504906892776489, Validation Loss: 0.8468016137050677\n",
            "Epoch 7, Loss: 0.8052093386650085, Validation Loss: 0.8370066088966176\n",
            "Epoch 8, Loss: 1.043770432472229, Validation Loss: 0.8814972095851656\n",
            "Epoch 9, Loss: 0.8163450360298157, Validation Loss: 0.8643755724158468\n",
            "Epoch 10, Loss: 1.1290944814682007, Validation Loss: 1.0357853445825698\n",
            "Epoch 11, Loss: 0.9226245880126953, Validation Loss: 0.8465531853180898\n",
            "Epoch 12, Loss: 0.8088032603263855, Validation Loss: 0.8345404433298714\n",
            "Epoch 13, Loss: 0.9490882754325867, Validation Loss: 0.833029832266554\n",
            "Epoch 14, Loss: 0.8714950084686279, Validation Loss: 0.8376499157917651\n",
            "Epoch 15, Loss: 1.2536146640777588, Validation Loss: 0.8341337774373307\n",
            "Epoch 16, Loss: 0.9752706289291382, Validation Loss: 0.8363865734655646\n",
            "Epoch 17, Loss: 0.887362003326416, Validation Loss: 0.8332943448537513\n",
            "Epoch 18, Loss: 0.6435747742652893, Validation Loss: 0.8351640542851219\n",
            "Epoch 19, Loss: 0.8521527051925659, Validation Loss: 0.832971133763277\n",
            "Epoch 20, Loss: 0.8731516003608704, Validation Loss: 0.8365388376803338\n",
            "Epoch 21, Loss: 0.9071750044822693, Validation Loss: 0.833778600904006\n",
            "Epoch 22, Loss: 0.8129894137382507, Validation Loss: 0.8324607022200958\n",
            "Epoch 23, Loss: 0.6642811894416809, Validation Loss: 0.8420590124552763\n",
            "Epoch 24, Loss: 0.7875949144363403, Validation Loss: 0.8340738574160805\n",
            "Epoch 25, Loss: 1.027191400527954, Validation Loss: 0.8360679700404783\n",
            "Epoch 26, Loss: 1.695176362991333, Validation Loss: 0.8329525099524969\n",
            "Epoch 27, Loss: 0.884066641330719, Validation Loss: 0.8362759072569352\n",
            "Epoch 28, Loss: 0.720045268535614, Validation Loss: 0.8499905527392521\n",
            "Epoch 29, Loss: 0.8684872984886169, Validation Loss: 0.8326734070536457\n",
            "Epoch 30, Loss: 1.12921941280365, Validation Loss: 0.8358359495295754\n",
            "Epoch 31, Loss: 1.1110150814056396, Validation Loss: 0.8325092188919647\n",
            "Epoch 32, Loss: 0.795211672782898, Validation Loss: 0.8329079641571527\n",
            "Epoch 33, Loss: 0.7813290953636169, Validation Loss: 0.8337077030652686\n",
            "Epoch 34, Loss: 0.700468122959137, Validation Loss: 0.833439510834368\n",
            "Epoch 35, Loss: 0.7368060946464539, Validation Loss: 0.8336676623247847\n",
            "Epoch 36, Loss: 1.9274011850357056, Validation Loss: 0.8325384017787402\n",
            "Epoch 37, Loss: 0.8268871307373047, Validation Loss: 0.8323161775552774\n",
            "Epoch 38, Loss: 0.7947309017181396, Validation Loss: 0.8322596964956839\n",
            "Epoch 39, Loss: 0.9650161862373352, Validation Loss: 0.8331551717806466\n",
            "Epoch 40, Loss: 0.7955939173698425, Validation Loss: 0.8415033424956889\n",
            "Epoch 41, Loss: 0.6774548292160034, Validation Loss: 0.8329820617844786\n",
            "Epoch 42, Loss: 0.8458738923072815, Validation Loss: 0.834604371197616\n",
            "Epoch 43, Loss: 0.7403166890144348, Validation Loss: 0.8325161488750313\n",
            "Epoch 44, Loss: 1.0124131441116333, Validation Loss: 0.8329660417158392\n",
            "Epoch 45, Loss: 0.6885097622871399, Validation Loss: 0.8356224539913709\n",
            "Epoch 46, Loss: 1.0201412439346313, Validation Loss: 0.8327308138714561\n",
            "Epoch 47, Loss: 1.1502705812454224, Validation Loss: 0.8332093259956264\n",
            "Epoch 48, Loss: 0.8402281999588013, Validation Loss: 0.8328766053235983\n",
            "Epoch 49, Loss: 0.6121309399604797, Validation Loss: 0.8347545175612727\n",
            "Epoch 50, Loss: 0.7894935607910156, Validation Loss: 0.8327187963678867\n",
            "Epoch 51, Loss: 0.7664162516593933, Validation Loss: 0.8344841976709003\n",
            "Epoch 52, Loss: 0.7940581440925598, Validation Loss: 0.8325135655040983\n",
            "Epoch 53, Loss: 0.6962308287620544, Validation Loss: 0.8324151869061627\n",
            "Epoch 54, Loss: 0.8418976664543152, Validation Loss: 0.8326004041901117\n",
            "Epoch 55, Loss: 0.8621276021003723, Validation Loss: 0.8323086281365986\n",
            "Epoch 56, Loss: 0.8087445497512817, Validation Loss: 0.8364958431147322\n",
            "Epoch 57, Loss: 0.9026851058006287, Validation Loss: 0.8369607133201405\n",
            "Epoch 58, Loss: 0.7437645792961121, Validation Loss: 0.833315796489957\n",
            "Epoch 59, Loss: 0.83794766664505, Validation Loss: 0.8366924230056473\n",
            "Epoch 60, Loss: 0.6751400828361511, Validation Loss: 0.8410013197343561\n",
            "Epoch 61, Loss: 0.8437607288360596, Validation Loss: 0.83597536026677\n",
            "Epoch 62, Loss: 0.7648748755455017, Validation Loss: 0.8324856592130058\n",
            "Epoch 63, Loss: 1.009188175201416, Validation Loss: 0.834066040908234\n",
            "Epoch 64, Loss: 1.0430389642715454, Validation Loss: 0.8330784326867212\n",
            "Epoch 65, Loss: 0.7331253886222839, Validation Loss: 0.8339079686358005\n",
            "Epoch 66, Loss: 0.7553256750106812, Validation Loss: 0.833314978623692\n",
            "Epoch 67, Loss: 0.9530993103981018, Validation Loss: 0.8336996232407002\n",
            "Epoch 68, Loss: 0.695773184299469, Validation Loss: 0.8349964347066758\n",
            "Epoch 69, Loss: 0.8672723174095154, Validation Loss: 0.8379032687295841\n",
            "Epoch 70, Loss: 0.8910210132598877, Validation Loss: 0.8374026402642455\n",
            "Epoch 71, Loss: 1.1150602102279663, Validation Loss: 0.832263249385206\n",
            "Epoch 72, Loss: 1.217699408531189, Validation Loss: 0.8321750254570683\n",
            "Epoch 73, Loss: 0.7109121680259705, Validation Loss: 0.8348852260203301\n",
            "Epoch 74, Loss: 0.5196661353111267, Validation Loss: 0.8327577121650116\n",
            "Epoch 75, Loss: 1.1049003601074219, Validation Loss: 0.8329715147803102\n",
            "Epoch 76, Loss: 0.9869219660758972, Validation Loss: 0.8351468828660024\n",
            "Epoch 77, Loss: 0.9644744396209717, Validation Loss: 0.8327100631556933\n",
            "Epoch 78, Loss: 0.8856048583984375, Validation Loss: 0.8330841230440743\n",
            "Epoch 79, Loss: 0.7295742034912109, Validation Loss: 0.8351358959946451\n",
            "Epoch 80, Loss: 0.705543041229248, Validation Loss: 0.8331922602049912\n",
            "Epoch 81, Loss: 0.949216902256012, Validation Loss: 0.8356348193144496\n",
            "Epoch 82, Loss: 0.6901794075965881, Validation Loss: 0.8324286417116092\n",
            "Epoch 83, Loss: 0.8268623352050781, Validation Loss: 0.8331206953978236\n",
            "Epoch 84, Loss: 0.7608257532119751, Validation Loss: 0.8340430048447621\n",
            "Epoch 85, Loss: 0.6825358271598816, Validation Loss: 0.8323711971693402\n",
            "Epoch 86, Loss: 0.7231536507606506, Validation Loss: 0.832515100889568\n",
            "Epoch 87, Loss: 0.7499154210090637, Validation Loss: 0.8361976388134534\n",
            "Epoch 88, Loss: 0.9287530183792114, Validation Loss: 0.836569036109538\n",
            "Epoch 89, Loss: 0.85435551404953, Validation Loss: 0.8357127701179891\n",
            "Epoch 90, Loss: 0.8093864321708679, Validation Loss: 0.8327313948281204\n",
            "Epoch 91, Loss: 0.8687447309494019, Validation Loss: 0.8326977793174454\n",
            "Epoch 92, Loss: 1.1077295541763306, Validation Loss: 0.8334802623036541\n",
            "Epoch 93, Loss: 1.0370008945465088, Validation Loss: 0.8323407822017428\n",
            "Epoch 94, Loss: 0.6311878561973572, Validation Loss: 0.8322819652436655\n",
            "Epoch 95, Loss: 0.9162652492523193, Validation Loss: 0.8328304554842696\n",
            "Epoch 96, Loss: 1.0367302894592285, Validation Loss: 0.8345804667171044\n",
            "Epoch 97, Loss: 0.9862772822380066, Validation Loss: 0.8324162688436387\n",
            "Epoch 98, Loss: 0.8203985095024109, Validation Loss: 0.8333651280101342\n",
            "Epoch 99, Loss: 1.0065165758132935, Validation Loss: 0.8329371055470237\n",
            "Epoch 100, Loss: 0.778006911277771, Validation Loss: 0.8326852834677394\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve output weights from the trained model\n",
        "output_weights = net.output_weight.detach().cpu().numpy().flatten()  # Flatten to make indexing easier\n",
        "\n",
        "# ... (Symbolic equation retrieval code remains the same)\n",
        "\n",
        "# Define the function for each symbolic transformation\n",
        "# Assuming funcs are applied cyclically to input features dx, dv, v\n",
        "input_features = ['dx', 'dv', 'v']\n",
        "symbolic_equations = []\n",
        "\n",
        "for i, layer in enumerate(net.hidden_layers):\n",
        "    if isinstance(layer, SymbolicLayer):\n",
        "        func_type = type(layer.function).__name__\n",
        "        feature = input_features[layer.slice_index]\n",
        "        # Depending on the function type, format the symbolic expression\n",
        "        if func_type == 'Square':\n",
        "            expr = f\"({feature}^2)\"\n",
        "        elif func_type == 'Identity':\n",
        "            expr = f\"({feature})\"\n",
        "        else:\n",
        "            expr = f\"({feature})\"  # Default case, add more cases if you have more function types\n",
        "    elif isinstance(layer, InteractionLayer):\n",
        "        feature1 = input_features[layer.slice_indices[0]]\n",
        "        feature2 = input_features[layer.slice_indices[1]]\n",
        "        expr = f\"({feature1} * {feature2})\"\n",
        "    else:\n",
        "        continue  # Skip layers that are not SymbolicLayer or InteractionLayer\n",
        "\n",
        "    # Multiply by the corresponding weight and add to the list\n",
        "    weight = output_weights[len(symbolic_equations)]  # Use the current length of symbolic_equations as the index\n",
        "    symbolic_equations.append(f\"{weight:.3f} * {expr}\")\n",
        "\n",
        "# Retrieve the bias term from the model and extract the scalar value\n",
        "bias = net.output_weight.detach().cpu().numpy()[-1].item()\n",
        "\n",
        "\n",
        "# Combine all parts into a single equation string\n",
        "final_equation = \" + \".join(symbolic_equations) + f\" + {bias:.3f}\"\n",
        "print(\"Symbolic Equation:\", final_equation)\n",
        "\n",
        "# Calculate the mean squared error on the validation set\n",
        "net.eval()\n",
        "val_mse = 0.0\n",
        "with torch.no_grad():\n",
        "    for inputs, targets in validation_loader:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        outputs = net(inputs)\n",
        "        val_mse += torch.mean((outputs - targets)**2).item() * inputs.size(0)\n",
        "val_mse /= len(validation_loader.dataset)\n",
        "\n",
        "print(f'Validation MSE: {val_mse:.4f}')"
      ],
      "metadata": {
        "id": "oddL_wqSEs8u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve output weights from the trained model\n",
        "output_weights = net.output_weight.detach().cpu().numpy().flatten()  # Flatten to make indexing easier\n",
        "\n",
        "# Define the function for each symbolic transformation\n",
        "# Assuming funcs are applied cyclically to input features dx, dv, v\n",
        "input_features = ['dx', 'dv', 'v']\n",
        "symbolic_equations = []\n",
        "\n",
        "for i, layer in enumerate(net.hidden_layers):\n",
        "    if isinstance(layer, SymbolicLayer):\n",
        "        func_type = type(layer.function).__name__\n",
        "        feature = input_features[layer.slice_index]\n",
        "        # Depending on the function type, format the symbolic expression\n",
        "        if func_type == 'Square':\n",
        "            expr = f\"({feature}^2)\"\n",
        "        elif func_type == 'Identity':\n",
        "            expr = f\"({feature})\"\n",
        "        else:\n",
        "            expr = f\"({feature})\"  # Default case, add more cases if you have more function types\n",
        "    elif isinstance(layer, InteractionLayer):\n",
        "        feature1 = input_features[layer.slice_indices[0]]\n",
        "        feature2 = input_features[layer.slice_indices[1]]\n",
        "        expr = f\"({feature1} * {feature2})\"\n",
        "    else:\n",
        "        continue  # Skip layers that are not SymbolicLayer or InteractionLayer\n",
        "\n",
        "    # Multiply by the corresponding weight and add to the list\n",
        "    weight = output_weights[len(symbolic_equations)]  # Use the current length of symbolic_equations as the index\n",
        "    symbolic_equations.append(f\"{weight:.3f} * {expr}\")\n",
        "\n",
        "# Retrieve the bias term from the model and extract the scalar value\n",
        "bias = net.output_weight.detach().cpu().numpy()[-1].item()\n",
        "\n",
        "# Combine all parts into a single equation string\n",
        "final_equation = \" + \".join(symbolic_equations) + f\" + {bias:.3f}\"\n",
        "print(\"Symbolic Equation:\", final_equation)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-3H4wkmg86ck",
        "outputId": "ae00c0d5-89bc-4f08-888c-76bd971b162c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Symbolic Equation: -0.011 * (dx^2) + -0.127 * (dv) + 0.058 * (v^2) + 0.449 * (dx) + -0.009 * (dv^2) + -0.003 * (dx * dv) + 0.061 * (dv * v) + 0.061\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "class InteractionLayer(nn.Module):\n",
        "    \"\"\" Layer to create interaction terms between features. \"\"\"\n",
        "    def __init__(self, slice_indices):\n",
        "        super().__init__()\n",
        "        self.slice_indices = slice_indices\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x[:, self.slice_indices[0]] * x[:, self.slice_indices[1]]\n",
        "\n",
        "class SymbolicLayer(nn.Module):\n",
        "    \"\"\" Simple symbolic layer applying a predefined function to a sliced input. \"\"\"\n",
        "    def __init__(self, function, slice_index):\n",
        "        super().__init__()\n",
        "        self.function = function\n",
        "        self.slice_index = slice_index\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.function(x[:, self.slice_index:self.slice_index+1])\n",
        "\n",
        "class Square(nn.Module):\n",
        "    def forward(self, x):\n",
        "        return x ** 2\n",
        "\n",
        "class Identity(nn.Module):\n",
        "    def forward(self, x):\n",
        "        return x\n",
        "class InteractionLayer(nn.Module):\n",
        "    \"\"\"Layer to create interaction terms between features.\"\"\"\n",
        "    def __init__(self, slice_indices):\n",
        "        super().__init__()\n",
        "        self.slice_indices = slice_indices\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x[:, self.slice_indices[0]:self.slice_indices[0]+1] * x[:, self.slice_indices[1]:self.slice_indices[1]+1]\n",
        "\n",
        "class SymbolicLayer(nn.Module):\n",
        "    \"\"\"Simple symbolic layer applying a predefined function to a sliced input.\"\"\"\n",
        "    def __init__(self, function, slice_index):\n",
        "        super().__init__()\n",
        "        self.function = function\n",
        "        self.slice_index = slice_index\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.function(x[:, self.slice_index:self.slice_index+1])\n",
        "class SymbolicNet(nn.Module):\n",
        "    def __init__(self, funcs, input_features):\n",
        "        super().__init__()\n",
        "        self.hidden_layers = nn.ModuleList()\n",
        "        for i in range(5):\n",
        "            self.hidden_layers.append(SymbolicLayer(funcs[i % len(funcs)], i % input_features))\n",
        "        self.hidden_layers.append(InteractionLayer([0, 1]))  # Interaction between the first and second feature\n",
        "        self.hidden_layers.append(InteractionLayer([1, 2]))  # Interaction between the second and third feature\n",
        "        self.output_weight = nn.Parameter(torch.randn(7, 1) * torch.sqrt(torch.tensor(2.0 / 7)))\n",
        "\n",
        "    def forward(self, x):\n",
        "        outputs = []\n",
        "        for layer in self.hidden_layers:\n",
        "            outputs.append(layer(x))\n",
        "        x = torch.cat(outputs, dim=1)  # Change dim to 1\n",
        "        x = torch.matmul(x, self.output_weight)\n",
        "        return x\n",
        "\n",
        "funcs = [Square(), Identity(), Square(), Identity(), Square()]\n",
        "net = SymbolicNet(funcs=funcs, input_features=3)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "net.to(device)\n",
        "\n",
        "criterion = nn.L1Loss()\n",
        "optimizer = torch.optim.AdamW(net.parameters(), lr=0.001, weight_decay=0.01)\n",
        "scheduler = torch.optim.lr_scheduler.CyclicLR(\n",
        "    optimizer,\n",
        "    base_lr=0.0001,\n",
        "    max_lr=0.01,\n",
        "    step_size_up=10,\n",
        "    mode='exp_range',\n",
        "    gamma=0.85,\n",
        "    cycle_momentum=False  # Disable cycle_momentum for AdamW\n",
        ")\n",
        "\n",
        "\n",
        "# ... (Previous code remains the same)\n",
        "\n",
        "num_epochs = 100\n",
        "reg_lambda = 0.01  # Regularization strength\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    net.train()\n",
        "    train_loss = 0.0\n",
        "    for inputs, targets in train_loader:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = net(inputs)\n",
        "\n",
        "        # Calculate the regularization term for each parameter tensor\n",
        "        reg_term = 0\n",
        "        for param in net.parameters():\n",
        "            reg_term += l12_smooth_reg(param)\n",
        "\n",
        "        # Calculate the mean squared error loss\n",
        "        loss = torch.mean((outputs - targets)**2) + reg_lambda * reg_term\n",
        "\n",
        "        train_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    scheduler.step()\n",
        "\n",
        "    net.eval()\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in validation_loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = net(inputs)\n",
        "            val_loss += torch.mean((outputs - targets)**2).item() * inputs.size(0)\n",
        "\n",
        "    train_loss /= len(train_loader.dataset)\n",
        "    val_loss /= len(validation_loader.dataset)\n",
        "\n",
        "    print(f'Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}')\n",
        "\n",
        "# Retrieve output weights from the trained model\n",
        "output_weights = net.output_weight.detach().cpu().numpy().flatten()  # Flatten to make indexing easier\n",
        "\n",
        "# ... (Symbolic equation retrieval code remains the same)\n",
        "\n",
        "# Combine all parts into a single equation string\n",
        "final_equation = \" + \".join(symbolic_equations) + f\" + {bias:.3f}\"\n",
        "print(\"Symbolic Equation:\", final_equation)\n",
        "\n",
        "# Calculate the mean squared error on the validation set\n",
        "net.eval()\n",
        "val_mse = 0.0\n",
        "with torch.no_grad():\n",
        "    for inputs, targets in validation_loader:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        outputs = net(inputs)\n",
        "        val_mse += torch.mean((outputs - targets)**2).item() * inputs.size(0)\n",
        "val_mse /= len(validation_loader.dataset)\n",
        "\n",
        "print(f'Validation MSE: {val_mse:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tbg5KVx5868J",
        "outputId": "5810d288-726d-433a-c081-26d6ddcf78e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Train Loss: 60008.2837, Validation Loss: 60015.8095\n",
            "Epoch 2, Train Loss: 35486.3396, Validation Loss: 20525.5486\n",
            "Epoch 3, Train Loss: 8329.5473, Validation Loss: 2411.6750\n",
            "Epoch 4, Train Loss: 727.2214, Validation Loss: 159.3366\n",
            "Epoch 5, Train Loss: 80.1644, Validation Loss: 61.1614\n",
            "Epoch 6, Train Loss: 50.0448, Validation Loss: 46.2297\n",
            "Epoch 7, Train Loss: 37.9890, Validation Loss: 36.7087\n",
            "Epoch 8, Train Loss: 30.2522, Validation Loss: 30.2672\n",
            "Epoch 9, Train Loss: 25.1868, Validation Loss: 26.0594\n",
            "Epoch 10, Train Loss: 22.0460, Validation Loss: 22.6565\n",
            "Epoch 11, Train Loss: 19.7584, Validation Loss: 20.8999\n",
            "Epoch 12, Train Loss: 18.1975, Validation Loss: 19.1874\n",
            "Epoch 13, Train Loss: 16.9659, Validation Loss: 17.9404\n",
            "Epoch 14, Train Loss: 15.9941, Validation Loss: 16.9695\n",
            "Epoch 15, Train Loss: 15.2516, Validation Loss: 16.3008\n",
            "Epoch 16, Train Loss: 14.6623, Validation Loss: 15.6810\n",
            "Epoch 17, Train Loss: 14.1509, Validation Loss: 15.2091\n",
            "Epoch 18, Train Loss: 13.7614, Validation Loss: 14.8922\n",
            "Epoch 19, Train Loss: 13.4616, Validation Loss: 14.5576\n",
            "Epoch 20, Train Loss: 13.2262, Validation Loss: 14.3237\n",
            "Epoch 21, Train Loss: 13.0031, Validation Loss: 14.2003\n",
            "Epoch 22, Train Loss: 12.8053, Validation Loss: 13.8580\n",
            "Epoch 23, Train Loss: 12.5213, Validation Loss: 13.5265\n",
            "Epoch 24, Train Loss: 12.1838, Validation Loss: 13.0577\n",
            "Epoch 25, Train Loss: 11.7641, Validation Loss: 12.7537\n",
            "Epoch 26, Train Loss: 11.3133, Validation Loss: 12.0390\n",
            "Epoch 27, Train Loss: 10.7894, Validation Loss: 11.4198\n",
            "Epoch 28, Train Loss: 10.2566, Validation Loss: 10.8485\n",
            "Epoch 29, Train Loss: 9.7106, Validation Loss: 10.2532\n",
            "Epoch 30, Train Loss: 9.1629, Validation Loss: 9.6868\n",
            "Epoch 31, Train Loss: 8.5801, Validation Loss: 8.9152\n",
            "Epoch 32, Train Loss: 8.0249, Validation Loss: 8.3842\n",
            "Epoch 33, Train Loss: 7.5354, Validation Loss: 7.8277\n",
            "Epoch 34, Train Loss: 7.0870, Validation Loss: 7.3444\n",
            "Epoch 35, Train Loss: 6.6696, Validation Loss: 6.9462\n",
            "Epoch 36, Train Loss: 6.2446, Validation Loss: 6.6229\n",
            "Epoch 37, Train Loss: 5.8843, Validation Loss: 6.1938\n",
            "Epoch 38, Train Loss: 5.5499, Validation Loss: 5.7432\n",
            "Epoch 39, Train Loss: 5.2270, Validation Loss: 5.4216\n",
            "Epoch 40, Train Loss: 4.9030, Validation Loss: 5.0297\n",
            "Epoch 41, Train Loss: 4.6164, Validation Loss: 4.7248\n",
            "Epoch 42, Train Loss: 4.3425, Validation Loss: 4.4355\n",
            "Epoch 43, Train Loss: 4.0670, Validation Loss: 4.1514\n",
            "Epoch 44, Train Loss: 3.8342, Validation Loss: 3.9314\n",
            "Epoch 45, Train Loss: 3.6029, Validation Loss: 3.6284\n",
            "Epoch 46, Train Loss: 3.3805, Validation Loss: 3.4115\n",
            "Epoch 47, Train Loss: 3.1668, Validation Loss: 3.2067\n",
            "Epoch 48, Train Loss: 2.9859, Validation Loss: 3.0079\n",
            "Epoch 49, Train Loss: 2.8025, Validation Loss: 2.9088\n",
            "Epoch 50, Train Loss: 2.6565, Validation Loss: 2.7357\n",
            "Epoch 51, Train Loss: 2.5064, Validation Loss: 2.5051\n",
            "Epoch 52, Train Loss: 2.3696, Validation Loss: 2.3867\n",
            "Epoch 53, Train Loss: 2.2618, Validation Loss: 2.2580\n",
            "Epoch 54, Train Loss: 2.1501, Validation Loss: 2.1975\n",
            "Epoch 55, Train Loss: 2.0756, Validation Loss: 2.0476\n",
            "Epoch 56, Train Loss: 1.9763, Validation Loss: 1.9616\n",
            "Epoch 57, Train Loss: 1.9012, Validation Loss: 1.8995\n",
            "Epoch 58, Train Loss: 1.8361, Validation Loss: 1.8094\n",
            "Epoch 59, Train Loss: 1.7781, Validation Loss: 1.7711\n",
            "Epoch 60, Train Loss: 1.7272, Validation Loss: 1.7031\n",
            "Epoch 61, Train Loss: 1.6784, Validation Loss: 1.6921\n",
            "Epoch 62, Train Loss: 1.6375, Validation Loss: 1.6765\n",
            "Epoch 63, Train Loss: 1.6067, Validation Loss: 1.6306\n",
            "Epoch 64, Train Loss: 1.5753, Validation Loss: 1.5584\n",
            "Epoch 65, Train Loss: 1.5490, Validation Loss: 1.5416\n",
            "Epoch 66, Train Loss: 1.5216, Validation Loss: 1.5485\n",
            "Epoch 67, Train Loss: 1.5041, Validation Loss: 1.4971\n",
            "Epoch 68, Train Loss: 1.4861, Validation Loss: 1.4923\n",
            "Epoch 69, Train Loss: 1.4773, Validation Loss: 1.4649\n",
            "Epoch 70, Train Loss: 1.4530, Validation Loss: 1.4845\n",
            "Epoch 71, Train Loss: 1.4466, Validation Loss: 1.4579\n",
            "Epoch 72, Train Loss: 1.4404, Validation Loss: 1.4307\n",
            "Epoch 73, Train Loss: 1.4324, Validation Loss: 1.4513\n",
            "Epoch 74, Train Loss: 1.4223, Validation Loss: 1.4229\n",
            "Epoch 75, Train Loss: 1.4116, Validation Loss: 1.4394\n",
            "Epoch 76, Train Loss: 1.4086, Validation Loss: 1.4657\n",
            "Epoch 77, Train Loss: 1.4032, Validation Loss: 1.4036\n",
            "Epoch 78, Train Loss: 1.3918, Validation Loss: 1.4190\n",
            "Epoch 79, Train Loss: 1.3912, Validation Loss: 1.4061\n",
            "Epoch 80, Train Loss: 1.3800, Validation Loss: 1.3962\n",
            "Epoch 81, Train Loss: 1.3814, Validation Loss: 1.4410\n",
            "Epoch 82, Train Loss: 1.3850, Validation Loss: 1.4023\n",
            "Epoch 83, Train Loss: 1.3826, Validation Loss: 1.3878\n",
            "Epoch 84, Train Loss: 1.3741, Validation Loss: 1.3916\n",
            "Epoch 85, Train Loss: 1.3739, Validation Loss: 1.3806\n",
            "Epoch 86, Train Loss: 1.3652, Validation Loss: 1.4148\n",
            "Epoch 87, Train Loss: 1.3606, Validation Loss: 1.4203\n",
            "Epoch 88, Train Loss: 1.3598, Validation Loss: 1.3868\n",
            "Epoch 89, Train Loss: 1.3636, Validation Loss: 1.4062\n",
            "Epoch 90, Train Loss: 1.3531, Validation Loss: 1.3848\n",
            "Epoch 91, Train Loss: 1.3598, Validation Loss: 1.3773\n",
            "Epoch 92, Train Loss: 1.3615, Validation Loss: 1.4235\n",
            "Epoch 93, Train Loss: 1.3514, Validation Loss: 1.3753\n",
            "Epoch 94, Train Loss: 1.3545, Validation Loss: 1.4395\n",
            "Epoch 95, Train Loss: 1.3538, Validation Loss: 1.4094\n",
            "Epoch 96, Train Loss: 1.3483, Validation Loss: 1.3674\n",
            "Epoch 97, Train Loss: 1.3465, Validation Loss: 1.3821\n",
            "Epoch 98, Train Loss: 1.3567, Validation Loss: 1.3984\n",
            "Epoch 99, Train Loss: 1.3439, Validation Loss: 1.3667\n",
            "Epoch 100, Train Loss: 1.3459, Validation Loss: 1.3926\n",
            "Symbolic Equation: -0.011 * (dx^2) + -0.127 * (dv) + 0.058 * (v^2) + 0.449 * (dx) + -0.009 * (dv^2) + -0.003 * (dx * dv) + 0.061 * (dv * v) + 0.061\n",
            "Validation MSE: 1.3926\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "class InteractionLayer(nn.Module):\n",
        "    \"\"\" Layer to create interaction terms between features. \"\"\"\n",
        "    def __init__(self, slice_indices):\n",
        "        super().__init__()\n",
        "        self.slice_indices = slice_indices\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x[:, self.slice_indices[0]] * x[:, self.slice_indices[1]]\n",
        "\n",
        "class SymbolicLayer(nn.Module):\n",
        "    \"\"\" Simple symbolic layer applying a predefined function to a sliced input. \"\"\"\n",
        "    def __init__(self, function, slice_index):\n",
        "        super().__init__()\n",
        "        self.function = function\n",
        "        self.slice_index = slice_index\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.function(x[:, self.slice_index:self.slice_index+1])\n",
        "\n",
        "class Square(nn.Module):\n",
        "    def forward(self, x):\n",
        "        return x ** 2\n",
        "\n",
        "class Identity(nn.Module):\n",
        "    def forward(self, x):\n",
        "        return x\n",
        "class InteractionLayer(nn.Module):\n",
        "    \"\"\"Layer to create interaction terms between features.\"\"\"\n",
        "    def __init__(self, slice_indices):\n",
        "        super().__init__()\n",
        "        self.slice_indices = slice_indices\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x[:, self.slice_indices[0]:self.slice_indices[0]+1] * x[:, self.slice_indices[1]:self.slice_indices[1]+1]\n",
        "\n",
        "class SymbolicLayer(nn.Module):\n",
        "    \"\"\"Simple symbolic layer applying a predefined function to a sliced input.\"\"\"\n",
        "    def __init__(self, function, slice_index):\n",
        "        super().__init__()\n",
        "        self.function = function\n",
        "        self.slice_index = slice_index\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.function(x[:, self.slice_index:self.slice_index+1])\n",
        "class SymbolicNet(nn.Module):\n",
        "    def __init__(self, funcs, input_features):\n",
        "        super().__init__()\n",
        "        self.hidden_layers = nn.ModuleList()\n",
        "        for i in range(5):\n",
        "            self.hidden_layers.append(SymbolicLayer(funcs[i % len(funcs)], i % input_features))\n",
        "        self.hidden_layers.append(InteractionLayer([0, 1]))  # Interaction between the first and second feature\n",
        "        self.hidden_layers.append(InteractionLayer([1, 2]))  # Interaction between the second and third feature\n",
        "        self.output_weight = nn.Parameter(torch.randn(7, 1) * torch.sqrt(torch.tensor(2.0 / 7)))\n",
        "\n",
        "    def forward(self, x):\n",
        "        outputs = []\n",
        "        for layer in self.hidden_layers:\n",
        "            outputs.append(layer(x))\n",
        "        x = torch.cat(outputs, dim=1)  # Change dim to 1\n",
        "        x = torch.matmul(x, self.output_weight)\n",
        "        return x\n",
        "\n",
        "funcs = [Square(), Identity(), Square(), Identity(), Square()]\n",
        "net = SymbolicNet(funcs=funcs, input_features=3)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "net.to(device)\n",
        "\n",
        "criterion = nn.L1Loss()\n",
        "optimizer = torch.optim.AdamW(net.parameters(), lr=0.001, weight_decay=0.01)\n",
        "scheduler = torch.optim.lr_scheduler.CyclicLR(\n",
        "    optimizer,\n",
        "    base_lr=0.0001,\n",
        "    max_lr=0.01,\n",
        "    step_size_up=10,\n",
        "    mode='exp_range',\n",
        "    gamma=0.85,\n",
        "    cycle_momentum=False  # Disable cycle_momentum for AdamW\n",
        ")\n",
        "\n",
        "num_epochs = 100\n",
        "reg_lambda = 0.01  # Regularization strength\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    net.train()\n",
        "    for inputs, targets in train_loader:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = net(inputs)\n",
        "\n",
        "        # Calculate the regularization term for each parameter tensor\n",
        "        reg_term = 0\n",
        "        for param in net.parameters():\n",
        "            reg_term += l12_smooth_reg(param)\n",
        "\n",
        "        # Add the regularization term to the loss\n",
        "        loss = criterion(outputs, targets) + reg_lambda * reg_term\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    scheduler.step()\n",
        "\n",
        "    net.eval()\n",
        "    validation_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in validation_loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = net(inputs)\n",
        "            validation_loss += criterion(outputs, targets).item()\n",
        "    print(f'Epoch {epoch+1}, Loss: {loss.item()}, Validation Loss: {validation_loss / len(validation_loader)}')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3lfuFNtjEt_m",
        "outputId": "89e36789-df23-4ae9-be1c-0f3f7d78adf0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 63.82646179199219, Validation Loss: 67.69113704826259\n",
            "Epoch 2, Loss: 4.2512712478637695, Validation Loss: 5.456262558321409\n",
            "Epoch 3, Loss: 2.156994104385376, Validation Loss: 2.346244288396232\n",
            "Epoch 4, Loss: 1.6452672481536865, Validation Loss: 1.5758306738696521\n",
            "Epoch 5, Loss: 0.7156260013580322, Validation Loss: 1.0090347528457642\n",
            "Epoch 6, Loss: 0.7288315892219543, Validation Loss: 0.8495021462440491\n",
            "Epoch 7, Loss: 0.7917455434799194, Validation Loss: 0.847599225708201\n",
            "Epoch 8, Loss: 0.7810760140419006, Validation Loss: 0.8494267750389969\n",
            "Epoch 9, Loss: 0.9398188591003418, Validation Loss: 0.8616395079636876\n",
            "Epoch 10, Loss: 0.8598364591598511, Validation Loss: 0.8331296315676049\n",
            "Epoch 11, Loss: 0.8229178190231323, Validation Loss: 0.89651595716235\n",
            "Epoch 12, Loss: 0.7914537191390991, Validation Loss: 0.8683391009704976\n",
            "Epoch 13, Loss: 0.9909659028053284, Validation Loss: 0.9027230045463466\n",
            "Epoch 14, Loss: 1.0297654867172241, Validation Loss: 0.8383057894586008\n",
            "Epoch 15, Loss: 0.8063573837280273, Validation Loss: 0.8340843712227254\n",
            "Epoch 16, Loss: 0.6667882800102234, Validation Loss: 0.8412689385534842\n",
            "Epoch 17, Loss: 0.8691961765289307, Validation Loss: 0.8474193879320652\n",
            "Epoch 18, Loss: 0.7007489800453186, Validation Loss: 0.8327151884006548\n",
            "Epoch 19, Loss: 0.8069955706596375, Validation Loss: 0.8344918708258038\n",
            "Epoch 20, Loss: 0.771131157875061, Validation Loss: 0.8324656041362618\n",
            "Epoch 21, Loss: 0.8310303688049316, Validation Loss: 0.8323441193073611\n",
            "Epoch 22, Loss: 0.67426598072052, Validation Loss: 0.8423314456698261\n",
            "Epoch 23, Loss: 0.6550709009170532, Validation Loss: 0.8398265612276294\n",
            "Epoch 24, Loss: 0.7201734781265259, Validation Loss: 0.842659545095661\n",
            "Epoch 25, Loss: 0.7512363791465759, Validation Loss: 0.8330581459818007\n",
            "Epoch 26, Loss: 0.920997142791748, Validation Loss: 0.834534414206879\n",
            "Epoch 27, Loss: 0.7701740264892578, Validation Loss: 0.8347782696349711\n",
            "Epoch 28, Loss: 0.7296440601348877, Validation Loss: 0.83399009553692\n",
            "Epoch 29, Loss: 0.7241800427436829, Validation Loss: 0.8360819582697712\n",
            "Epoch 30, Loss: 0.7573312520980835, Validation Loss: 0.8369103149522709\n",
            "Epoch 31, Loss: 0.5745301246643066, Validation Loss: 0.8346897371207611\n",
            "Epoch 32, Loss: 0.6876638531684875, Validation Loss: 0.8329153883306286\n",
            "Epoch 33, Loss: 0.6923584938049316, Validation Loss: 0.8332076570655726\n",
            "Epoch 34, Loss: 0.8465421199798584, Validation Loss: 0.8349511185778847\n",
            "Epoch 35, Loss: 0.9834759831428528, Validation Loss: 0.8369678968115698\n",
            "Epoch 36, Loss: 0.7999460101127625, Validation Loss: 0.8359354417535323\n",
            "Epoch 37, Loss: 0.6707957983016968, Validation Loss: 0.8327190008344529\n",
            "Epoch 38, Loss: 0.8133617639541626, Validation Loss: 0.8327685843540144\n",
            "Epoch 39, Loss: 0.7327802777290344, Validation Loss: 0.8340723280665241\n",
            "Epoch 40, Loss: 0.8036785125732422, Validation Loss: 0.8323606833626952\n",
            "Epoch 41, Loss: 1.0170944929122925, Validation Loss: 0.8326247907892058\n",
            "Epoch 42, Loss: 0.8320696353912354, Validation Loss: 0.8328664581986922\n",
            "Epoch 43, Loss: 0.8404619097709656, Validation Loss: 0.8341656048086625\n",
            "Epoch 44, Loss: 0.7288950085639954, Validation Loss: 0.833228631110131\n",
            "Epoch 45, Loss: 0.7802345156669617, Validation Loss: 0.8343354908725883\n",
            "Epoch 46, Loss: 0.6000511646270752, Validation Loss: 0.8390391408642636\n",
            "Epoch 47, Loss: 1.0471117496490479, Validation Loss: 0.8351274653326107\n",
            "Epoch 48, Loss: 0.8903178572654724, Validation Loss: 0.8344945424719702\n",
            "Epoch 49, Loss: 0.8511325716972351, Validation Loss: 0.8324466053443619\n",
            "Epoch 50, Loss: 0.9706723690032959, Validation Loss: 0.8328102653539633\n",
            "Epoch 51, Loss: 0.5696237087249756, Validation Loss: 0.8324666106248204\n",
            "Epoch 52, Loss: 0.8671127557754517, Validation Loss: 0.8331317931790895\n",
            "Epoch 53, Loss: 0.6966066956520081, Validation Loss: 0.832322657862796\n",
            "Epoch 54, Loss: 0.7380303144454956, Validation Loss: 0.8326822956906089\n",
            "Epoch 55, Loss: 0.7661381959915161, Validation Loss: 0.8329301438754118\n",
            "Epoch 56, Loss: 1.2404338121414185, Validation Loss: 0.8381938730614095\n",
            "Epoch 57, Loss: 0.7888363003730774, Validation Loss: 0.8374961184549935\n",
            "Epoch 58, Loss: 1.014411449432373, Validation Loss: 0.834996171389954\n",
            "Epoch 59, Loss: 0.9097996950149536, Validation Loss: 0.8350489773327792\n",
            "Epoch 60, Loss: 0.9511591196060181, Validation Loss: 0.8325556494012664\n",
            "Epoch 61, Loss: 1.4391059875488281, Validation Loss: 0.83244788646698\n",
            "Epoch 62, Loss: 1.0227832794189453, Validation Loss: 0.8327442996109589\n",
            "Epoch 63, Loss: 0.6756830811500549, Validation Loss: 0.8322391706176951\n",
            "Epoch 64, Loss: 0.7400174140930176, Validation Loss: 0.8329977174348469\n",
            "Epoch 65, Loss: 0.7398759126663208, Validation Loss: 0.8324091464658326\n",
            "Epoch 66, Loss: 1.1499723196029663, Validation Loss: 0.8325894090193736\n",
            "Epoch 67, Loss: 1.1514289379119873, Validation Loss: 0.8355759653864028\n",
            "Epoch 68, Loss: 0.9925035834312439, Validation Loss: 0.8324389834947223\n",
            "Epoch 69, Loss: 0.6202818155288696, Validation Loss: 0.8346918496904494\n",
            "Epoch 70, Loss: 0.8862987160682678, Validation Loss: 0.8329202985461754\n",
            "Epoch 71, Loss: 0.6082448363304138, Validation Loss: 0.8330104381223268\n",
            "Epoch 72, Loss: 0.9930775165557861, Validation Loss: 0.8336727136298071\n",
            "Epoch 73, Loss: 1.5501247644424438, Validation Loss: 0.8372327670266356\n",
            "Epoch 74, Loss: 0.6823680996894836, Validation Loss: 0.832544397704209\n",
            "Epoch 75, Loss: 0.771192729473114, Validation Loss: 0.8379726334463192\n",
            "Epoch 76, Loss: 0.6501173377037048, Validation Loss: 0.8337869319734694\n",
            "Epoch 77, Loss: 0.8256428241729736, Validation Loss: 0.8355354183836828\n",
            "Epoch 78, Loss: 0.603147029876709, Validation Loss: 0.8332986718491663\n",
            "Epoch 79, Loss: 0.6310264468193054, Validation Loss: 0.8326248141783702\n",
            "Epoch 80, Loss: 0.8131837248802185, Validation Loss: 0.8322730773611914\n",
            "Epoch 81, Loss: 0.7037011384963989, Validation Loss: 0.8322041389308398\n",
            "Epoch 82, Loss: 0.6566816568374634, Validation Loss: 0.8327255935608586\n",
            "Epoch 83, Loss: 0.8280941247940063, Validation Loss: 0.837043272543557\n",
            "Epoch 84, Loss: 0.9185319542884827, Validation Loss: 0.8330293335492098\n",
            "Epoch 85, Loss: 1.4547309875488281, Validation Loss: 0.8345584145075158\n",
            "Epoch 86, Loss: 0.9333820939064026, Validation Loss: 0.8326534628868103\n",
            "Epoch 87, Loss: 1.105047345161438, Validation Loss: 0.832892903044254\n",
            "Epoch 88, Loss: 0.7929171919822693, Validation Loss: 0.8349865116650546\n",
            "Epoch 89, Loss: 0.7273121476173401, Validation Loss: 0.8325558561313001\n",
            "Epoch 90, Loss: 0.7753167748451233, Validation Loss: 0.8345144858843163\n",
            "Epoch 91, Loss: 0.8080750703811646, Validation Loss: 0.83427045164229\n",
            "Epoch 92, Loss: 0.602815568447113, Validation Loss: 0.8349651588669306\n",
            "Epoch 93, Loss: 0.6289492249488831, Validation Loss: 0.8397596576545812\n",
            "Epoch 94, Loss: 0.9186128973960876, Validation Loss: 0.8323306773282304\n",
            "Epoch 95, Loss: 0.8123803734779358, Validation Loss: 0.8435472989384132\n",
            "Epoch 96, Loss: 0.7488154768943787, Validation Loss: 0.8324109512039378\n",
            "Epoch 97, Loss: 0.7135149836540222, Validation Loss: 0.8340203052834619\n",
            "Epoch 98, Loss: 1.1102124452590942, Validation Loss: 0.8333183587351932\n",
            "Epoch 99, Loss: 0.6811643838882446, Validation Loss: 0.8327698737760133\n",
            "Epoch 100, Loss: 0.7068089842796326, Validation Loss: 0.8326608885692645\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve output weights from the trained model\n",
        "output_weights = net.output_weight.detach().cpu().numpy().flatten()  # Flatten to make indexing easier\n",
        "\n",
        "# ... (Symbolic equation retrieval code remains the same)\n",
        "\n",
        "# Define the function for each symbolic transformation\n",
        "# Assuming funcs are applied cyclically to input features dx, dv, v\n",
        "input_features = ['dx', 'dv', 'v']\n",
        "symbolic_equations = []\n",
        "\n",
        "for i, layer in enumerate(net.hidden_layers):\n",
        "    if isinstance(layer, SymbolicLayer):\n",
        "        func_type = type(layer.function).__name__\n",
        "        feature = input_features[layer.slice_index]\n",
        "        # Depending on the function type, format the symbolic expression\n",
        "        if func_type == 'Square':\n",
        "            expr = f\"({feature}^2)\"\n",
        "        elif func_type == 'Identity':\n",
        "            expr = f\"({feature})\"\n",
        "        else:\n",
        "            expr = f\"({feature})\"  # Default case, add more cases if you have more function types\n",
        "    elif isinstance(layer, InteractionLayer):\n",
        "        feature1 = input_features[layer.slice_indices[0]]\n",
        "        feature2 = input_features[layer.slice_indices[1]]\n",
        "        expr = f\"({feature1} * {feature2})\"\n",
        "    else:\n",
        "        continue  # Skip layers that are not SymbolicLayer or InteractionLayer\n",
        "\n",
        "    # Multiply by the corresponding weight and add to the list\n",
        "    weight = output_weights[len(symbolic_equations)]  # Use the current length of symbolic_equations as the index\n",
        "    symbolic_equations.append(f\"{weight:.3f} * {expr}\")\n",
        "\n",
        "# Retrieve the bias term from the model and extract the scalar value\n",
        "bias = net.output_weight.detach().cpu().numpy()[-1].item()\n",
        "\n",
        "\n",
        "# Combine all parts into a single equation string\n",
        "final_equation = \" + \".join(symbolic_equations) + f\" + {bias:.3f}\"\n",
        "print(\"Symbolic Equation:\", final_equation)\n",
        "\n",
        "# Calculate the mean squared error on the validation set\n",
        "net.eval()\n",
        "val_mse = 0.0\n",
        "with torch.no_grad():\n",
        "    for inputs, targets in validation_loader:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        outputs = net(inputs)\n",
        "        val_mse += torch.mean((outputs - targets)**2).item() * inputs.size(0)\n",
        "val_mse /= len(validation_loader.dataset)\n",
        "\n",
        "print(f'Validation MSE: {val_mse:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fXwQ-KnrFsnw",
        "outputId": "ec8d338b-1029-405d-9512-46acbbd8e409"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Symbolic Equation: -0.010 * (dx^2) + -0.127 * (dv) + 0.058 * (v^2) + 0.449 * (dx) + -0.009 * (dv^2) + -0.004 * (dx * dv) + 0.060 * (dv * v) + 0.060\n",
            "Validation MSE: 2.1778\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#making Symbolic Net more complex\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "class InteractionLayer(nn.Module):\n",
        "    \"\"\"Layer to create interaction terms between features.\"\"\"\n",
        "    def __init__(self, slice_indices):\n",
        "        super().__init__()\n",
        "        self.slice_indices = slice_indices\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x[:, self.slice_indices[0]] * x[:, self.slice_indices[1]]\n",
        "\n",
        "class SymbolicLayer(nn.Module):\n",
        "    \"\"\"Simple symbolic layer applying a predefined function to a sliced input.\"\"\"\n",
        "    def __init__(self, function, slice_index):\n",
        "        super().__init__()\n",
        "        self.function = function\n",
        "        self.slice_index = slice_index\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.function(x[:, self.slice_index:self.slice_index+1])\n",
        "\n",
        "class Square(nn.Module):\n",
        "    def forward(self, x):\n",
        "        return x ** 2\n",
        "\n",
        "class Identity(nn.Module):\n",
        "    def forward(self, x):\n",
        "        return x\n",
        "\n",
        "# Updated SymbolicNet class with num_hidden_layers and input_dim parameters\n",
        "class SymbolicNet(nn.Module):\n",
        "    def __init__(self, num_hidden_layers, funcs, input_dim=1):\n",
        "        super().__init__()\n",
        "        self.hidden_layers = nn.ModuleList()\n",
        "        for i in range(num_hidden_layers):\n",
        "            self.hidden_layers.append(SymbolicLayer(funcs[i % len(funcs)], i % input_dim))\n",
        "        self.hidden_layers.append(InteractionLayer([0, 1]))  # Interaction between the first and second feature\n",
        "        self.hidden_layers.append(InteractionLayer([1, 2]))  # Interaction between the second and third feature\n",
        "        self.output_weight = nn.Parameter(torch.randn(num_hidden_layers + 2, 1) * torch.sqrt(torch.tensor(2.0 / (num_hidden_layers + 2))))\n",
        "\n",
        "    def forward(self, x):\n",
        "        outputs = []\n",
        "        for layer in self.hidden_layers:\n",
        "            output = layer(x)\n",
        "            if output.dim() == 1:\n",
        "                output = output.unsqueeze(1)  # Add an extra dimension if the tensor has only one dimension\n",
        "            outputs.append(output)\n",
        "        x = torch.cat(outputs, dim=1)\n",
        "        x = torch.matmul(x, self.output_weight)\n",
        "        return x\n",
        "\n",
        "# Add the following lines here\n",
        "num_hidden_layers = 4  # Specify the desired number of hidden layers\n",
        "input_dim = 3  # Specify the dimensionality of the input data\n",
        "funcs = [Square(), Identity(), Square(), Identity(), Square()]\n",
        "net = SymbolicNet(num_hidden_layers, funcs=funcs, input_dim=input_dim)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "net.to(device)\n",
        "\n",
        "criterion = nn.L1Loss()\n",
        "optimizer = torch.optim.AdamW(net.parameters(), lr=0.001, weight_decay=0.01)\n",
        "scheduler = torch.optim.lr_scheduler.CyclicLR(\n",
        "    optimizer, base_lr=0.0001, max_lr=0.01, step_size_up=10, mode='exp_range', gamma=0.85, cycle_momentum=False\n",
        ")\n",
        "\n",
        "criterion = nn.L1Loss()\n",
        "optimizer = torch.optim.AdamW(net.parameters(), lr=0.001, weight_decay=0.01)\n",
        "scheduler = torch.optim.lr_scheduler.CyclicLR(\n",
        "    optimizer,\n",
        "    base_lr=0.0001,\n",
        "    max_lr=0.01,\n",
        "    step_size_up=10,\n",
        "    mode='exp_range',\n",
        "    gamma=0.85,\n",
        "    cycle_momentum=False  # Disable cycle_momentum for AdamW\n",
        ")\n",
        "\n",
        "num_epochs = 100\n",
        "reg_lambda = 0.01  # Regularization strength\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    net.train()\n",
        "    for inputs, targets in train_loader:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = net(inputs)\n",
        "\n",
        "        # Calculate the regularization term for each parameter tensor\n",
        "        reg_term = 0\n",
        "        for param in net.parameters():\n",
        "            reg_term += l12_smooth_reg(param)\n",
        "\n",
        "        # Add the regularization term to the loss\n",
        "        loss = criterion(outputs, targets) + reg_lambda * reg_term\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    scheduler.step()\n",
        "\n",
        "    net.eval()\n",
        "    validation_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in validation_loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = net(inputs)\n",
        "            validation_loss += criterion(outputs, targets).item()\n",
        "    print(f'Epoch {epoch+1}, Loss: {loss.item()}, Validation Loss: {validation_loss / len(validation_loader)}')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CyarFbU1QgFp",
        "outputId": "96f04c72-9011-4ffa-8611-e5b00a974394"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 61.1967658996582, Validation Loss: 67.471561625034\n",
            "Epoch 2, Loss: 27.220211029052734, Validation Loss: 32.745859894571424\n",
            "Epoch 3, Loss: 6.92686128616333, Validation Loss: 7.726172012618825\n",
            "Epoch 4, Loss: 1.65268874168396, Validation Loss: 2.121576909777484\n",
            "Epoch 5, Loss: 0.8786890506744385, Validation Loss: 0.9822866750668876\n",
            "Epoch 6, Loss: 0.9550784230232239, Validation Loss: 0.9087459656256663\n",
            "Epoch 7, Loss: 0.8265520930290222, Validation Loss: 0.8580439392524429\n",
            "Epoch 8, Loss: 0.9738342761993408, Validation Loss: 0.8357940542547009\n",
            "Epoch 9, Loss: 0.6605714559555054, Validation Loss: 0.8357748894751826\n",
            "Epoch 10, Loss: 0.9118329286575317, Validation Loss: 0.8580577818653251\n",
            "Epoch 11, Loss: 0.6513704657554626, Validation Loss: 0.938098128083386\n",
            "Epoch 12, Loss: 0.7552004456520081, Validation Loss: 0.8412725578380537\n",
            "Epoch 13, Loss: 1.1613324880599976, Validation Loss: 0.8432387143750734\n",
            "Epoch 14, Loss: 0.896597683429718, Validation Loss: 0.8370290526860877\n",
            "Epoch 15, Loss: 0.7294631004333496, Validation Loss: 0.8401195180567005\n",
            "Epoch 16, Loss: 0.7373648285865784, Validation Loss: 0.8647731331330312\n",
            "Epoch 17, Loss: 0.8469635248184204, Validation Loss: 0.845643404918381\n",
            "Epoch 18, Loss: 1.2096773386001587, Validation Loss: 0.8361964286128177\n",
            "Epoch 19, Loss: 0.8119890689849854, Validation Loss: 0.8328134628790843\n",
            "Epoch 20, Loss: 0.7488901615142822, Validation Loss: 0.8386372644690019\n",
            "Epoch 21, Loss: 0.7158334851264954, Validation Loss: 0.8356446669071536\n",
            "Epoch 22, Loss: 0.7420681715011597, Validation Loss: 0.8346548555772516\n",
            "Epoch 23, Loss: 0.6810757517814636, Validation Loss: 0.8471237519119359\n",
            "Epoch 24, Loss: 0.9632822871208191, Validation Loss: 0.8322210402428349\n",
            "Epoch 25, Loss: 0.7964959740638733, Validation Loss: 0.8340665788590154\n",
            "Epoch 26, Loss: 0.6656888723373413, Validation Loss: 0.8328311277341239\n",
            "Epoch 27, Loss: 0.7826176881790161, Validation Loss: 0.8333608613738531\n",
            "Epoch 28, Loss: 1.0512359142303467, Validation Loss: 0.8331449778774117\n",
            "Epoch 29, Loss: 0.8460630178451538, Validation Loss: 0.8325794286365751\n",
            "Epoch 30, Loss: 1.2833112478256226, Validation Loss: 0.836361107192462\n",
            "Epoch 31, Loss: 0.8443771600723267, Validation Loss: 0.8339412235006501\n",
            "Epoch 32, Loss: 0.7874398231506348, Validation Loss: 0.8334071032608612\n",
            "Epoch 33, Loss: 1.0021363496780396, Validation Loss: 0.8452446226832233\n",
            "Epoch 34, Loss: 0.8627684116363525, Validation Loss: 0.83266323201264\n",
            "Epoch 35, Loss: 0.9882586002349854, Validation Loss: 0.8338129271434832\n",
            "Epoch 36, Loss: 0.7003584504127502, Validation Loss: 0.8322613510904433\n",
            "Epoch 37, Loss: 0.7362106442451477, Validation Loss: 0.8352750071996375\n",
            "Epoch 38, Loss: 0.7367478013038635, Validation Loss: 0.832924809636949\n",
            "Epoch 39, Loss: 0.8382871150970459, Validation Loss: 0.8330518376978138\n",
            "Epoch 40, Loss: 0.7128469347953796, Validation Loss: 0.8321814876568469\n",
            "Epoch 41, Loss: 0.7998316287994385, Validation Loss: 0.8375544985638389\n",
            "Epoch 42, Loss: 1.1704070568084717, Validation Loss: 0.8361971989462648\n",
            "Epoch 43, Loss: 0.8057894110679626, Validation Loss: 0.8335842980614191\n",
            "Epoch 44, Loss: 0.752629816532135, Validation Loss: 0.8345985261699821\n",
            "Epoch 45, Loss: 0.7057904005050659, Validation Loss: 0.8326592875432365\n",
            "Epoch 46, Loss: 0.7112852334976196, Validation Loss: 0.8348380553571484\n",
            "Epoch 47, Loss: 0.8598959445953369, Validation Loss: 0.8326302155663695\n",
            "Epoch 48, Loss: 0.8429492712020874, Validation Loss: 0.8367885486989082\n",
            "Epoch 49, Loss: 0.6608291864395142, Validation Loss: 0.8321789239026323\n",
            "Epoch 50, Loss: 0.7285987734794617, Validation Loss: 0.8332675227636024\n",
            "Epoch 51, Loss: 0.7383794188499451, Validation Loss: 0.8352352614644207\n",
            "Epoch 52, Loss: 0.8687029480934143, Validation Loss: 0.8323598873766163\n",
            "Epoch 53, Loss: 0.8077859878540039, Validation Loss: 0.8320282456241076\n",
            "Epoch 54, Loss: 0.7849574685096741, Validation Loss: 0.8420172786410851\n",
            "Epoch 55, Loss: 0.8424564599990845, Validation Loss: 0.8322561051272139\n",
            "Epoch 56, Loss: 0.5879088044166565, Validation Loss: 0.8324948239930069\n",
            "Epoch 57, Loss: 0.9926176071166992, Validation Loss: 0.8347974728934372\n",
            "Epoch 58, Loss: 0.9455984830856323, Validation Loss: 0.8335654773289645\n",
            "Epoch 59, Loss: 0.8266329765319824, Validation Loss: 0.8325242830228202\n",
            "Epoch 60, Loss: 0.7783632874488831, Validation Loss: 0.8346078003509135\n",
            "Epoch 61, Loss: 0.7917163372039795, Validation Loss: 0.8329410145554361\n",
            "Epoch 62, Loss: 0.7179451584815979, Validation Loss: 0.840923162955272\n",
            "Epoch 63, Loss: 0.6672593951225281, Validation Loss: 0.833466034901293\n",
            "Epoch 64, Loss: 0.9016209840774536, Validation Loss: 0.8351236347910724\n",
            "Epoch 65, Loss: 1.0124484300613403, Validation Loss: 0.8323063465613353\n",
            "Epoch 66, Loss: 0.8704776167869568, Validation Loss: 0.8353653172903424\n",
            "Epoch 67, Loss: 0.8093889951705933, Validation Loss: 0.8336206335055677\n",
            "Epoch 68, Loss: 0.7587206959724426, Validation Loss: 0.8341106455537337\n",
            "Epoch 69, Loss: 0.7309238910675049, Validation Loss: 0.8326911405672001\n",
            "Epoch 70, Loss: 1.1007118225097656, Validation Loss: 0.8320750797851176\n",
            "Epoch 71, Loss: 1.0333616733551025, Validation Loss: 0.8336871862411499\n",
            "Epoch 72, Loss: 0.8878063559532166, Validation Loss: 0.834329271618324\n",
            "Epoch 73, Loss: 1.2167936563491821, Validation Loss: 0.8342297726039645\n",
            "Epoch 74, Loss: 0.7921981811523438, Validation Loss: 0.8320734470705443\n",
            "Epoch 75, Loss: 0.8959951400756836, Validation Loss: 0.8325915049903\n",
            "Epoch 76, Loss: 0.987930417060852, Validation Loss: 0.8329926804651188\n",
            "Epoch 77, Loss: 0.7641182541847229, Validation Loss: 0.8398268441610699\n",
            "Epoch 78, Loss: 0.8653795123100281, Validation Loss: 0.8325266800349271\n",
            "Epoch 79, Loss: 0.9041856527328491, Validation Loss: 0.8323358682137502\n",
            "Epoch 80, Loss: 0.9336018562316895, Validation Loss: 0.8327309489250183\n",
            "Epoch 81, Loss: 1.333550214767456, Validation Loss: 0.8395929449721228\n",
            "Epoch 82, Loss: 0.8786724209785461, Validation Loss: 0.8341924057731146\n",
            "Epoch 83, Loss: 0.5178163051605225, Validation Loss: 0.8329729913156244\n",
            "Epoch 84, Loss: 0.6986805200576782, Validation Loss: 0.8328984530666207\n",
            "Epoch 85, Loss: 0.674812912940979, Validation Loss: 0.8319875454600854\n",
            "Epoch 86, Loss: 0.9716825485229492, Validation Loss: 0.834937295581721\n",
            "Epoch 87, Loss: 0.8256463408470154, Validation Loss: 0.8352878810484198\n",
            "Epoch 88, Loss: 0.7551003098487854, Validation Loss: 0.83226731230941\n",
            "Epoch 89, Loss: 0.8154448866844177, Validation Loss: 0.8333587073072602\n",
            "Epoch 90, Loss: 0.8309997916221619, Validation Loss: 0.8333373326289503\n",
            "Epoch 91, Loss: 0.9561386108398438, Validation Loss: 0.83281898649433\n",
            "Epoch 92, Loss: 0.8238281011581421, Validation Loss: 0.832113145272943\n",
            "Epoch 93, Loss: 0.9343870878219604, Validation Loss: 0.8329659602310084\n",
            "Epoch 94, Loss: 1.0835455656051636, Validation Loss: 0.8322803657266158\n",
            "Epoch 95, Loss: 1.4605700969696045, Validation Loss: 0.8325229339961764\n",
            "Epoch 96, Loss: 0.8815320730209351, Validation Loss: 0.8338797484772115\n",
            "Epoch 97, Loss: 0.8818434476852417, Validation Loss: 0.83353754538524\n",
            "Epoch 98, Loss: 0.6612966656684875, Validation Loss: 0.8350210099280635\n",
            "Epoch 99, Loss: 0.6856553554534912, Validation Loss: 0.8324355037906502\n",
            "Epoch 100, Loss: 0.8365671634674072, Validation Loss: 0.833557168139687\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve output weights from the trained model\n",
        "output_weights = net.output_weight.detach().cpu().numpy().flatten()  # Flatten to make indexing easier\n",
        "\n",
        "# ... (Symbolic equation retrieval code remains the same)\n",
        "\n",
        "# Define the function for each symbolic transformation\n",
        "# Assuming funcs are applied cyclically to input features dx, dv, v\n",
        "input_features = ['dx', 'dv', 'v']\n",
        "symbolic_equations = []\n",
        "\n",
        "for i, layer in enumerate(net.hidden_layers):\n",
        "    if isinstance(layer, SymbolicLayer):\n",
        "        func_type = type(layer.function).__name__\n",
        "        feature = input_features[layer.slice_index]\n",
        "        # Depending on the function type, format the symbolic expression\n",
        "        if func_type == 'Square':\n",
        "            expr = f\"({feature}^2)\"\n",
        "        elif func_type == 'Identity':\n",
        "            expr = f\"({feature})\"\n",
        "        else:\n",
        "            expr = f\"({feature})\"  # Default case, add more cases if you have more function types\n",
        "    elif isinstance(layer, InteractionLayer):\n",
        "        feature1 = input_features[layer.slice_indices[0]]\n",
        "        feature2 = input_features[layer.slice_indices[1]]\n",
        "        expr = f\"({feature1} * {feature2})\"\n",
        "    else:\n",
        "        continue  # Skip layers that are not SymbolicLayer or InteractionLayer\n",
        "\n",
        "    # Multiply by the corresponding weight and add to the list\n",
        "    weight = output_weights[len(symbolic_equations)]  # Use the current length of symbolic_equations as the index\n",
        "    symbolic_equations.append(f\"{weight:.3f} * {expr}\")\n",
        "\n",
        "# Retrieve the bias term from the model and extract the scalar value\n",
        "bias = net.output_weight.detach().cpu().numpy()[-1].item()\n",
        "\n",
        "\n",
        "# Combine all parts into a single equation string\n",
        "final_equation = \" + \".join(symbolic_equations) + f\" + {bias:.3f}\"\n",
        "print(\"Symbolic Equation:\", final_equation)\n",
        "\n",
        "# Calculate the mean squared error on the validation set\n",
        "net.eval()\n",
        "val_mse = 0.0\n",
        "with torch.no_grad():\n",
        "    for inputs, targets in validation_loader:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        outputs = net(inputs)\n",
        "        val_mse += torch.mean((outputs - targets)**2).item() * inputs.size(0)\n",
        "val_mse /= len(validation_loader.dataset)\n",
        "\n",
        "print(f'Validation MSE: {val_mse:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uujSyarrTxuX",
        "outputId": "a9445532-ce3e-4745-cbc8-1c7eee513d42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Symbolic Equation: -0.011 * (dx^2) + -0.142 * (dv) + 0.058 * (v^2) + 0.447 * (dx) + -0.004 * (dx * dv) + 0.066 * (dv * v) + 0.066\n",
            "Validation MSE: 2.2763\n"
          ]
        }
      ]
    }
  ]
}